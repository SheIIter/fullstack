import gradio as gr
import os
import json
import csv
import mimetypes
import requests
import re
import base64
import subprocess
import tempfile
from pathlib import Path
from dotenv import load_dotenv
import io
from PIL import Image, ImageDraw, ImageFont
import textwrap
from datetime import datetime
from tqdm import tqdm 

from langchain_upstage import (
    UpstageDocumentParseLoader,
    UpstageEmbeddings,
    ChatUpstage,
    UpstageGroundednessCheck,
)
from operator import itemgetter
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# Groundedness ì²´í¬ìš© ì»¨í…ìŠ¤íŠ¸ ì§ë ¬í™” ìœ í‹¸
def docs_to_text(docs):
    try:
        return "\n\n---\n\n".join(getattr(d, "page_content", str(d)) for d in docs)
    except Exception:
        return str(docs)

# Groundedness ì»¨í…ìŠ¤íŠ¸ ë¹Œë”: ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ + ê³„ì•½/ì§ˆë¬¸ ì›ë¬¸ ê²°í•©
def build_grounded_context_for_contract(contract_text: str) -> str:
    try:
        retrieved = RETRIEVER.invoke(contract_text) if RETRIEVER else []
    except Exception:
        retrieved = []
    retrieved_text = docs_to_text(retrieved)
    return f"[ì°¸ê³  ìë£Œ]\n{retrieved_text}\n\n[ê³„ì•½ì„œ]\n{contract_text}"

def build_grounded_context_for_question(question_text: str) -> str:
    try:
        retrieved = RETRIEVER.invoke(question_text) if RETRIEVER else []
    except Exception:
        retrieved = []
    retrieved_text = docs_to_text(retrieved)
    return f"[ì°¸ê³  ìë£Œ]\n{retrieved_text}\n\n[ì§ˆë¬¸]\n{question_text}"

# ì„ íƒ ì˜ì¡´ì„± (HTML -> PNG ë³€í™˜ìš©)
HTML2IMAGE_AVAILABLE = False
try:
    from html2image import Html2Image
    HTML2IMAGE_AVAILABLE = True
except Exception:
    HTML2IMAGE_AVAILABLE = False

# ì„ íƒ ì˜ì¡´ì„± (Markdown -> HTML ë³€í™˜) - FIXED
MARKDOWN_AVAILABLE = False
try:
    import markdown2
    MARKDOWN_AVAILABLE = True
except Exception:
    try:
        import markdown
        MARKDOWN_AVAILABLE = True
    except Exception:
        MARKDOWN_AVAILABLE = False

# í™˜ê²½ ë³€ìˆ˜
try:
    if load_dotenv():
        print("ğŸ”‘ API í‚¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    DEEPL_API_KEY = os.getenv("DEEPL_API_KEY")
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")
except:
    DEEPL_API_KEY = None
    GOOGLE_API_KEY = None
    UPSTAGE_API_KEY = None

# API ì—”ë“œí¬ì¸íŠ¸
TTS_API_URL = f"https://texttospeech.googleapis.com/v1/text:synthesize?key={GOOGLE_API_KEY}" if GOOGLE_API_KEY else None
STT_API_URL = f"https://speech.googleapis.com/v1/speech:recognize?key={GOOGLE_API_KEY}" if GOOGLE_API_KEY else None
DEEPL_API_URL = "https://api-free.deepl.com/v2/translate"

# ë°ì´í„° ê²½ë¡œ ì„¤ì •
EASYLAW_QA_PATH = "./data/easylaw_qa_data.json"
SPECIAL_CLAUSES_PATH = "./data/íŠ¹ì•½ë¬¸êµ¬ í•©ë³¸_utf8bom.csv"
LAW_PARSED_PATH = "./data/ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•(ë²•ë¥ )(ì œ19356í˜¸)_parsed.json"
DEFAULTER_LIST_PATH = "./data/ìƒìŠµì±„ë¬´ë¶ˆì´í–‰ì.CSV"
CHROMA_DB_PATH = "./chroma_db_real_estate_gradio"

# ë‹¤êµ­ì–´ í°íŠ¸ ìë™ ë‹¤ìš´ë¡œë“œ ë¡œì§, TTFë§Œìœ¼ë¡œ ì •í™•í•œ ë§í¬ë¡œ ìˆ˜ì • ì§„í–‰.
FONTS_DIR = Path("./fonts")
FONT_URLS = {
    # Noto Sans (ë¼í‹´/í‚¤ë¦´/ê¸°ë³¸ì˜ë¬¸)
    "NotoSans-Regular.ttf": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf",
    "NotoSans-Bold.ttf": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Bold.ttf",
    # Noto Sans KR (í•œêµ­ì–´)
    "NotoSansKR-Regular.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Korean/NotoSansCJKkr-Regular.otf",
    "NotoSansKR-Bold.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Korean/NotoSansCJKkr-Bold.otf",
    # Noto Sans JP (ì¼ë³¸ì–´) - TTF íŒŒì¼ë¡œ ìˆ˜ì •
    "NotoSansJP-Regular.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf",
    "NotoSansJP-Bold.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Bold.otf",
    # Noto Sans SC (ì¤‘êµ­ì–´ ê°„ì²´) - TTF íŒŒì¼ë¡œ ìˆ˜ì •
    "NotoSansSC-Regular.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Regular.otf",
    "NotoSansSC-Bold.ttf": "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/SimplifiedChinese/NotoSansCJKsc-Bold.otf",
    # Noto Color Emoji (ì´ëª¨ì§€ ì§€ì›)
    "NotoColorEmoji-Regular.ttf": "https://github.com/googlefonts/noto-emoji/raw/main/fonts/NotoColorEmoji.ttf",
    # Noto Sans (ìš°í¬ë¼ì´ë‚˜ì–´ í‚¤ë¦´ ë¬¸ì ì§€ì›) - ì¶”ê°€
    "NotoSans-{style}.ttf": "https://github.com/googlefonts/noto-fonts/raw/main/hinted/ttf/NotoSans/NotoSans-Regular.ttf"
}

def setup_fonts():
    """
    í•„ìš”í•œ ë‹¤êµ­ì–´ í°íŠ¸ë¥¼ ./fonts í´ë”ì— ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³ , OTF íŒŒì¼ì„ TTFë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print("ğŸ–‹ï¸ ë‹¤êµ­ì–´ í°íŠ¸ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    FONTS_DIR.mkdir(exist_ok=True)

    for font_name, url in FONT_URLS.items():
        font_path = FONTS_DIR / font_name
        if font_path.exists():
            print(f"  - '{font_name}' í°íŠ¸ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. (ê±´ë„ˆë›°ê¸°)")
            continue

        try:
            print(f"  - '{font_name}' í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì¤‘... ({url})")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            block_size = 1024
            
            with open(font_path, 'wb') as f, tqdm(
                total=total_size, unit='iB', unit_scale=True, desc=f"    {font_name}"
            ) as pbar:
                for data in response.iter_content(block_size):
                    pbar.update(len(data))
                    f.write(data)
            
            if total_size != 0 and pbar.n != total_size:
                 raise Exception("ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

            print(f"  ğŸ‰ '{font_name}' í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")

        except Exception as e:
            print(f"  âŒ '{font_name}' í°íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            if font_path.exists():
                font_path.unlink() # ì‹¤íŒ¨ ì‹œ ë¶ˆì™„ì „í•œ íŒŒì¼ ì‚­ì œ
    
    # OTF íŒŒì¼ì„ TTFë¡œ ë³€í™˜ ì‹œë„
    print("  - OTF íŒŒì¼ì„ TTFë¡œ ë³€í™˜ ì‹œë„ ì¤‘...")
    try:
        from fontTools.ttLib import TTFont
        from fontTools.ttx import makeOutputFileName
        
        for font_name in FONT_URLS.keys():
            if font_name.endswith('.otf'):
                otf_path = FONTS_DIR / font_name
                ttf_name = font_name.replace('.otf', '.ttf')
                ttf_path = FONTS_DIR / ttf_name
                
                if otf_path.exists() and not ttf_path.exists():
                    try:
                        print(f"    - '{font_name}' â†’ '{ttf_name}' ë³€í™˜ ì¤‘...")
                        font = TTFont(str(otf_path))
                        font.save(str(ttf_path))
                        print(f"    - '{ttf_name}' ë³€í™˜ ì™„ë£Œ!")
                    except Exception as e:
                        print(f"    - '{font_name}' ë³€í™˜ ì‹¤íŒ¨: {e}")
    except ImportError:
        print("    - fontToolsê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ OTFâ†’TTF ë³€í™˜ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print("    - pip install fonttoolsë¡œ ì„¤ì¹˜ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    print("âœ… ëª¨ë“  í°íŠ¸ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


def build_ai_brain_if_needed():
    """AIì˜ ì§€ì‹ ë² ì´ìŠ¤(Vector DB)ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœë‹ˆë‹¤."""
    if os.path.exists(CHROMA_DB_PATH):
        print(f"âœ… Vector DBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ({CHROMA_DB_PATH})")
        return
    print(f"âœ¨ AIì˜ ì§€ì‹ ë² ì´ìŠ¤(Vector DB)ë¥¼ ìƒˆë¡œ êµ¬ì¶•í•©ë‹ˆë‹¤...")
    all_documents = []
    
    # EasyLaw Q&A ë°ì´í„° ë¡œë“œ
    try:
        with open(EASYLAW_QA_PATH, 'r', encoding='utf-8') as f:
            for item in json.load(f):
                all_documents.append(Document(
                    page_content=f"ì‚¬ë¡€ ì§ˆë¬¸: {item['question']}\nì‚¬ë¡€ ë‹µë³€: {item['answer']}",
                    metadata={"source": "easylaw_qa"}
                ))
        print(f"  - EasyLaw QA ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(all_documents)}ê°œ ë¬¸ì„œ)")
    except FileNotFoundError:
        print(f"  [ê²½ê³ ] '{EASYLAW_QA_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë²•ë¥  ì¡°ë¬¸ ë°ì´í„° ë¡œë“œ
    try:
        with open(LAW_PARSED_PATH, 'r', encoding='utf-8') as f:
            law_text = json.load(f).get("text", "")
            all_documents.append(Document(
                page_content=law_text,
                metadata={"source": "housing_lease_law"}
            ))
        print(f"  - ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"  [ê²½ê³ ] '{LAW_PARSED_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # íŠ¹ì•½ ì¡°í•­ ë°ì´í„° ë¡œë“œ
    try:
        clauses_count = 0
        with open(SPECIAL_CLAUSES_PATH, 'r', encoding='utf-8-sig') as f:
            for row in csv.DictReader(f):
                if clause_content := row.get('íŠ¹ì•½ë‚´ìš©', '').strip():
                    all_documents.append(Document(
                        page_content=f"ê¶Œì¥ íŠ¹ì•½ ì¡°í•­ ì˜ˆì‹œ: {clause_content}",
                        metadata={"source": "special_clauses"}
                    ))
                    clauses_count += 1
        print(f"  - íŠ¹ì•½ ì¡°í•­ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({clauses_count}ê°œ)")
    except FileNotFoundError:
        print(f"  [ê²½ê³ ] '{SPECIAL_CLAUSES_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    if not all_documents:
        print("ğŸ”´ DBë¥¼ êµ¬ì¶•í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. RAG ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return
    
    print("  - í…ìŠ¤íŠ¸ ë¶„í•  ë° ì„ë² ë”© ì§„í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    split_docs = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100).split_documents(all_documents)
    Chroma.from_documents(
        documents=split_docs,
        embedding=UpstageEmbeddings(model="solar-embedding-1-large"),
        persist_directory=CHROMA_DB_PATH
    )
    print(f"ğŸ‰ Vector DB êµ¬ì¶• ì™„ë£Œ! ({CHROMA_DB_PATH})")

# ### MODIFIED FUNCTION ###: ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œëœ í°íŠ¸ë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
def get_multilingual_font(size=16, bold=False, lang_code='KO'):
    """
    ë¡œì»¬ ./fonts í´ë”ì— ë‹¤ìš´ë¡œë“œëœ Noto í°íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ë Œë”ë§ì„ ì§€ì›í•©ë‹ˆë‹¤.
    ì–¸ì–´ ì½”ë“œì— ë”°ë¼ ì ì ˆí•œ í°íŠ¸ íŒŒì¼ì„ ì„ íƒí•˜ì—¬ tofu í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    style = "Bold" if bold else "Regular"
    
    # ì–¸ì–´ ì½”ë“œì— ë”°ë¥¸ í°íŠ¸ íŒŒì¼ ë§¤í•‘ (TTF ìš°ì„ , OTF í´ë°±)
    font_map = {
        'KO': [f'NotoSansKR-{style}.ttf', f'NotoSansKR-{style}.otf'],
        'JA': [f'NotoSansJP-{style}.ttf', f'NotoSansJP-{style}.otf'],
        'ZH': [f'NotoSansSC-{style}.ttf', f'NotoSansSC-{style}.otf'],
        # ìš°í¬ë¼ì´ë‚˜ì–´(í‚¤ë¦´) - í‚¤ë¦´ ë¬¸ì ì§€ì› í°íŠ¸ ì¶”ê°€
        'UK': [f'NotoSans-{style}.ttf', f'NotoSansKR-{style}.ttf', f'NotoSansKR-{style}.otf'],
        'VI': [f'NotoSans-{style}.ttf'],
        'EN': [f'NotoSans-{style}.ttf'],
    }
    
    # ìš”ì²­ëœ ì–¸ì–´ì˜ í°íŠ¸ íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ NotoSans ì‚¬ìš©
    font_candidates = font_map.get(lang_code.upper(), [f'NotoSans-{style}.ttf'])
    
    # TTF íŒŒì¼ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ê¸°
    for font_filename in font_candidates:
        font_path = FONTS_DIR / font_filename
        if font_path.exists():
            try:
                return ImageFont.truetype(str(font_path), size)
            except Exception as e:
                print(f"âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ '{font_filename}': {e}")
                continue
    
    # ëª¨ë“  í›„ë³´ í°íŠ¸ê°€ ì‹¤íŒ¨í•œ ê²½ìš° í´ë°± ì‹œë„
    fallback_candidates = [
        FONTS_DIR / "NotoSans-Regular.ttf",
        FONTS_DIR / "NotoSans-Regular.otf"
    ]
    
    for fallback_path in fallback_candidates:
        if fallback_path.exists():
            try:
                print(f"âš ï¸ ê²½ê³ : ìš”ì²­ëœ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ '{fallback_path.name}'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return ImageFont.truetype(str(fallback_path), size)
            except Exception as e:
                print(f"âš ï¸ í´ë°± í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ '{fallback_path.name}': {e}")
                continue
    
    # ìµœí›„ì˜ ìˆ˜ë‹¨
    print("âŒ ëª¨ë“  í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨. PIL ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê¸€ìê°€ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    try:
        return ImageFont.load_default()
    except Exception:
        return None

def get_emoji_font(size=16):
    """
    ì´ëª¨ì§€ ì „ìš© í°íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    emoji_font_path = FONTS_DIR / "NotoColorEmoji-Regular.ttf"
    if emoji_font_path.exists():
        try:
            return ImageFont.truetype(str(emoji_font_path), size)
        except Exception as e:
            print(f"âš ï¸ ì´ëª¨ì§€ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    return None

def draw_text_with_emoji(draw, text, position, main_font, emoji_font, align='left', color='#000000'):
    """
    ì´ëª¨ì§€ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ í˜¼í•©í•˜ì—¬ ë Œë”ë§í•©ë‹ˆë‹¤.
    align: 'left', 'center', 'right'
    """
    if not emoji_font:
        # ì´ëª¨ì§€ í°íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ í°íŠ¸ë¡œ ë Œë”ë§
        if align == 'center':
            bbox = draw.textbbox((0, 0), text, font=main_font)
            x = position[0] - (bbox[2] - bbox[0]) // 2
            draw.text((x, position[1]), text, fill=color, font=main_font)
        else:
            draw.text(position, text, fill=color, font=main_font)
        return
    
    # ì´ëª¨ì§€ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬
    import re
    emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002600-\U000027BF\U0001F900-\U0001F9FF\U0001F018-\U0001F270]')
    
    # ì´ëª¨ì§€ ìœ„ì¹˜ ì°¾ê¸°
    emoji_positions = []
    for match in emoji_pattern.finditer(text):
        emoji_positions.append((match.start(), match.end(), match.group()))
    
    if not emoji_positions:
        # ì´ëª¨ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë Œë”ë§
        if align == 'center':
            bbox = draw.textbbox((0, 0), text, font=main_font)
            x = position[0] - (bbox[2] - bbox[0]) // 2
            draw.text((x, position[1]), text, fill=color, font=main_font)
        else:
            draw.text(position, text, fill=color, font=main_font)
        return
    
    # í…ìŠ¤íŠ¸ë¥¼ ì´ëª¨ì§€ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë¶„í• í•˜ì—¬ ë Œë”ë§
    current_x = position[0]
    if align == 'center':
        # ì „ì²´ í…ìŠ¤íŠ¸ ë„ˆë¹„ ê³„ì‚°
        total_width = 0
        last_end = 0
        for start, end, emoji in emoji_positions:
            # ì´ëª¨ì§€ ì•ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸
            if start > last_end:
                text_part = text[last_end:start]
                bbox = draw.textbbox((0, 0), text_part, font=main_font)
                total_width += bbox[2] - bbox[0]
            # ì´ëª¨ì§€
            bbox = draw.textbbox((0, 0), emoji, font=emoji_font)
            total_width += bbox[2] - bbox[0]
            last_end = end
        
        # ë§ˆì§€ë§‰ ì¼ë°˜ í…ìŠ¤íŠ¸
        if last_end < len(text):
            text_part = text[last_end:]
            bbox = draw.textbbox((0, 0), text_part, font=main_font)
            total_width += bbox[2] - bbox[0]
        
        current_x = position[0] - total_width // 2
    
    # ì‹¤ì œ ë Œë”ë§
    last_end = 0
    for start, end, emoji in emoji_positions:
        # ì´ëª¨ì§€ ì•ì˜ ì¼ë°˜ í…ìŠ¤íŠ¸
        if start > last_end:
            text_part = text[last_end:start]
            draw.text((current_x, position[1]), text_part, fill=color, font=main_font)
            bbox = draw.textbbox((0, 0), text_part, font=main_font)
            current_x += bbox[2] - bbox[0]
        
        # ì´ëª¨ì§€
        draw.text((current_x, position[1]), emoji, fill=color, font=emoji_font)
        bbox = draw.textbbox((0, 0), emoji, font=emoji_font)
        current_x += bbox[2] - bbox[0]
        last_end = end
    
    # ë§ˆì§€ë§‰ ì¼ë°˜ í…ìŠ¤íŠ¸
    if last_end < len(text):
        text_part = text[last_end:]
        draw.text((current_x, position[1]), text_part, fill=color, font=main_font)


def extract_text_from_file(file_path: str) -> tuple[str, str]:
    if not file_path or not os.path.exists(file_path):
        return "", "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        # Upstage ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì´ë¯¸ì§€ì™€ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. JPGë„ ì—¬ê¸°ì— í¬í•¨ë©ë‹ˆë‹¤.
        pages = UpstageDocumentParseLoader(file_path, ocr="force").load()
        extracted_text = "\n\n".join([p.page_content for p in pages if p.page_content])
        
        if not extracted_text.strip():
            return "", "íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ì¸ì‹ì´ ì–´ë µìŠµë‹ˆë‹¤."
            
        return extracted_text, "ì„±ê³µ"
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” êµ¬ì²´ì ì¸ ë©”ì‹œì§€ ë°˜í™˜
        error_message = f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n(ì„œë²„ ì˜¤ë¥˜: {str(e)})"
        print(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {error_message}")
        return "", error_message

def perform_rule_based_analysis(contract_text: str) -> dict:
    alerts, safety_score = [], 100
    try:
        # 1. ê¸°ì¡´ì˜ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (ìœ ì§€)
        categories = {
            "ë³´ì¦ê¸ˆ_ë°˜í™˜": {"keywords": ["ë³´ì¦ê¸ˆ", "ë°˜í™˜", "ì¦‰ì‹œ", "ê³„ì•½ì¢…ë£Œ"], "risk": "CRITICAL"},
            "ê¶Œë¦¬ê´€ê³„_ìœ ì§€": {"keywords": ["ê¶Œë¦¬ê´€ê³„", "ìµì¼", "ê·¼ì €ë‹¹", "ëŒ€í•­ë ¥"], "risk": "CRITICAL"},
            "ì „ì„¸ìê¸ˆëŒ€ì¶œ": {"keywords": ["ëŒ€ì¶œ", "ë¶ˆê°€", "ë¬´íš¨", "ì „ì„¸ìê¸ˆ"], "risk": "WARNING"},
            "ìˆ˜ì„ _ì˜ë¬´": {"keywords": ["ìˆ˜ì„ ", "í•˜ì", "íŒŒì†", "ìˆ˜ë¦¬"], "risk": "ADVISORY"},
            "íŠ¹ì•½ì‚¬í•­": {"keywords": ["íŠ¹ì•½", "ê¸°íƒ€ì‚¬í•­", "ì¶”ê°€ì¡°ê±´"], "risk": "ADVISORY"}
        }
        for cat_name, info in categories.items():
            display_name = cat_name.replace('_', ' ').title()
            keyword_count = sum(1 for kw in info['keywords'] if kw in contract_text)
            if keyword_count < len(info['keywords']) * 0.5:
                if info['risk'] == "CRITICAL":
                    safety_score -= 40
                    alerts.append(f"ğŸš¨ [ì¹˜ëª…ì !] {display_name}: ê´€ë ¨ ì¡°í•­ì´ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ ë¯¸ë¹„í•˜ì—¬ ì‹¬ê°í•œ ìœ„í—˜ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
                elif info['risk'] == "WARNING":
                    safety_score -= 20
                    alerts.append(f"âš ï¸ [ìœ„í—˜] {display_name}: ê´€ë ¨ ì¡°í•­ì´ ë¶€ì¡±í•˜ì—¬ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                else:
                    safety_score -= 10
                    alerts.append(f"ğŸ’¡ [ê¶Œì¥] {display_name}: ë¶„ìŸ ì˜ˆë°©ì„ ìœ„í•´ ê´€ë ¨ ì¡°í•­ ë³´ê°•ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            else:
                alerts.append(f"âœ… [{display_name}] ê´€ë ¨ ì¡°í•­ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

        safety_score = max(0, safety_score)

        # 2. ğŸ”¥ ì„ëŒ€ì¸ ì´ë¦„ ì¶”ì¶œ ë° ìƒìŠµ ì±„ë¬´ ë¶ˆì´í–‰ì ëª…ë‹¨ ì¡°íšŒ (í•µì‹¬ ê¸°ëŠ¥ ì¶”ê°€)
        print("  [ì„ëŒ€ì¸ ê²€ì‚¬] ì„ëŒ€ì¸ ì‹ ì› ì¡°íšŒ ì‹œì‘...")
        landlord_name = extract_landlord_name_robustly(contract_text)
        
        if landlord_name == "ì´ë¦„ ìë™ ì¶”ì¶œ ì‹¤íŒ¨":
            alerts.append("âš ï¸ [ì„ëŒ€ì¸ ê²€ì‚¬] ê³„ì•½ì„œì—ì„œ ì„ëŒ€ì¸ ì´ë¦„ì„ ìë™ìœ¼ë¡œ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            found_defaulter = False
            try:
                with open(DEFAULTER_LIST_PATH, 'r', encoding='utf-8-sig') as f:
                    # CSV íŒŒì¼ì˜ ëª¨ë“  í–‰ì„ ë¯¸ë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œí•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„± ì¦ëŒ€
                    defaulter_list = list(csv.DictReader(f))
                    for row in defaulter_list:
                        # ì´ë¦„ ë¹„êµ ì‹œ ê³µë°± ì œê±° í›„ ë¹„êµ
                        defaulter_name = row.get('ì„±ëª…', '').strip().replace(' ', '')
                        if landlord_name == defaulter_name:
                            safety_score = 0  # << ì¹˜ëª…ì  ìœ„í—˜ì´ë¯€ë¡œ ì•ˆì „ì ìˆ˜ 0ì ìœ¼ë¡œ ì¡°ì •
                            alerts.append(f"ğŸš¨ğŸš¨ğŸš¨ [ì¹˜ëª…ì  ìœ„í—˜!] ì„ëŒ€ì¸ '{landlord_name}'ì´(ê°€) ìƒìŠµ ì±„ë¬´ ë¶ˆì´í–‰ì ëª…ë‹¨ì— í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤! **ê³„ì•½ì„ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.**")
                            found_defaulter = True
                            break
                if not found_defaulter:
                    alerts.append(f"âœ… [ì„ëŒ€ì¸ ê²€ì‚¬] ì„ëŒ€ì¸('{landlord_name}')ì€(ëŠ”) ìƒìŠµ ì±„ë¬´ ë¶ˆì´í–‰ì ëª…ë‹¨ì— ì—†ìŠµë‹ˆë‹¤.")
            except FileNotFoundError:
                alerts.append(f"âš ï¸ [ì„ëŒ€ì¸ ê²€ì‚¬] ìƒìŠµ ì±„ë¬´ë¶ˆì´í–‰ì ëª…ë‹¨ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¡°íšŒê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ({DEFAULTER_LIST_PATH})")
            except Exception as e:
                alerts.append(f"âš ï¸ [ì„ëŒ€ì¸ ê²€ì‚¬] ëª…ë‹¨ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    except Exception as e:
        alerts.append(f"âš ï¸ ê·œì¹™ ê¸°ë°˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        safety_score = -1
    
    # ì•ˆì „ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¤‘ìš”í•œ ê²½ê³ ê°€ ìœ„ë¡œ ì˜¤ê²Œ í•¨
    alerts.sort(key=lambda x: ('ğŸš¨' not in x, 'âš ï¸' not in x, 'ğŸ’¡' not in x, 'âœ…' not in x))

    return {"alerts": alerts, "safety_score": safety_score}

def google_text_to_speech(text, lang_code="KO"):
    if not GOOGLE_API_KEY:
        return None, "Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ìŒì„± ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤."
    
    # íŠ¹ìˆ˜ë¬¸ì ì¼ë¶€ ì œê±° (ìŒì„± ë³€í™˜ í’ˆì§ˆ í–¥ìƒ)
    text = re.sub(r"[^\w\sê°€-í£.,!?]", "", text, flags=re.UNICODE)
    
    text_chunks = split_text_for_tts(text)
    
    voice_map = {
        "KO": {"languageCode": "ko-KR", "name": "ko-KR-Wavenet-A"},
        "EN": {"languageCode": "en-US", "name": "en-US-Wavenet-F"},
        "JA": {"languageCode": "ja-JP", "name": "ja-JP-Wavenet-A"},
        "ZH": {"languageCode": "cmn-CN", "name": "cmn-CN-Wavenet-A"},
        "UK": {"languageCode": "uk-UA", "name": "uk-UA-Wavenet-A"}, # ìš°í¬ë¼ì´ë‚˜ì–´
        "VI": {"languageCode": "vi-VN", "name": "vi-VN-Wavenet-A"}  # ë² íŠ¸ë‚¨ì–´
    }

    if lang_code.upper() not in voice_map:
        return None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ ì½”ë“œ: {lang_code}"
        
    try:
        # ê¸´ í…ìŠ¤íŠ¸ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì²­í¬ë§Œ ì²˜ë¦¬í•˜ì—¬ ìƒ˜í”Œ ì œê³µ (Gradioì—ì„œëŠ” ì „ì²´ë¥¼ ì²˜ë¦¬í•˜ë©´ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
        first_chunk = text_chunks[0] if text_chunks else ""
        if not first_chunk:
             return None, "ìŒì„±ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        request_body = {
            "input": {"text": first_chunk},
            "voice": voice_map[lang_code.upper()],
            "audioConfig": {"audioEncoding": "MP3", "speakingRate": 0.9, "pitch": -2}
        }
        
        response = requests.post(TTS_API_URL, data=json.dumps(request_body), timeout=30)

        if response.status_code == 200:
            audio_content = base64.b64decode(response.json()['audioContent'])
            # Gradioì—ì„œëŠ” ì„ì‹œ íŒŒì¼ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì •ì 
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp_file:
                tmp_file.write(audio_content)
                # ë©”ì‹œì§€ ê°œì„ 
                msg = "ìŒì„± ìƒì„± ì™„ë£Œ!"
                if len(text_chunks) > 1:
                    msg = f"ìŒì„± ìƒì„± ì™„ë£Œ ğŸµ "
                return tmp_file.name, msg
        else:
            return None, f"TTS API ì˜¤ë¥˜: {response.text}"
    except Exception as e:
        return None, f"TTS ìŠ¤í¬ë¦½íŠ¸ ì˜¤ë¥˜: {e}"

RETRIEVER = None
def initialize_retriever():
    """ì „ì—­ RAG ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global RETRIEVER
    if os.path.exists(CHROMA_DB_PATH):
        try:
            vectorstore = Chroma(
                persist_directory=CHROMA_DB_PATH,
                embedding_function=UpstageEmbeddings(model="solar-embedding-1-large")
            )
            RETRIEVER = vectorstore.as_retriever(search_kwargs={"k": 5})
            print("âœ… RAG ê²€ìƒ‰ê¸°(Retriever) ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ RAG ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    else:
        print("âš ï¸ Vector DB ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ RAG ê²€ìƒ‰ê¸°ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ğŸ¨ ì´ë¯¸ì§€ ìƒì„± ë° UI ê´€ë ¨ í•¨ìˆ˜ë“¤
# ğŸ¨ ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ìƒ‰ìƒ ë° í°íŠ¸ ì„¤ì • (PIL í´ë°±ìš©) - ì´ˆë¡ìƒ‰ í…Œë§ˆ
COLORS = {
    'bg': '#f0f9ff',
    'white': '#ffffff',
    'primary': '#10b981',
    'success': '#059669',
    'warning': '#f59e0b',
    'danger': '#ef4444',
    'text': '#1f2937',
    'muted': '#6b7280',
    'border': '#d1fae5',
    'accent': '#047857'
}

EMBED_HEAD = """
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<!-- ë‹¤êµ­ì–´ ì§€ì›ì„ ìœ„í•œ Noto Sans í°íŠ¸ íŒ¨ë°€ë¦¬ -->
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;500;700&family=Noto+Sans+KR:wght@400;500;700&family=Noto+Sans+JP:wght@400;500;700&family=Noto+Sans+SC:wght@400;500;700&display=swap" rel="stylesheet">
<style>
  :root {
    --card-bg: #ffffff;
    --bg: #f0fdf4;
    --border: #d1fae5;
    --text: #1f2937;
    --text-weak: #4b5563;
    --muted: #6b7280;
    --primary: #10b981;
    --primary-dark: #059669;
    --accent: #047857;
    --shadow: rgba(16, 185, 129, 0.1);
    --badge-bg: #ecfdf5;
    --badge-text: #065f46;
    --badge-border: #a7f3d0;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --card-bg: #0f172a;
      --bg: #020617;
      --border: #1e293b;
      --text: #e2e8f0;
      --text-weak: #cbd5e1;
      --muted: #94a3b8;
      --primary: #34d399;
      --primary-dark: #10b981;
      --accent: #059669;
      --shadow: rgba(16, 185, 129, 0.2);
      --badge-bg: #064e3b;
      --badge-text: #6ee7b7;
      --badge-border: #065f46;
    }
  }

  html, body {
    background: var(--bg);
    color: var(--text);
    font-family: 'Noto Sans KR', 'Noto Sans', 'Noto Sans JP', 'Noto Sans SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Malgun Gothic', sans-serif;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
    text-rendering: optimizeLegibility;
    line-height: 1.7;
  }
</style>
"""

# ğŸ”¥ FIXED: ì¤„ë°”ê¿ˆ ë¬¸ì œë¥¼ í•´ê²°í•œ CSS
DEFAULT_EMBED_CSS = """
.report-wrap { max-width: 980px; margin: 0 auto; padding: 32px; }
.report-card { background: var(--card-bg); border: 1px solid var(--border); border-radius: 20px; overflow: hidden; box-shadow: 0 10px 35px var(--shadow); }
.report-header { background: linear-gradient(135deg, var(--primary) 0%, var(--primary-dark) 100%); padding: 32px; color: #fff; }
.report-header h1 { margin: 0 0 8px 0; font-size: 26px; font-weight: 700; }
.report-header .meta { font-size: 14px; opacity: .9; }
.report-section { padding: 28px 32px; border-top: 1px solid var(--border); }
.report-section:last-child { border-bottom: none; }
.report-section h2 { margin: 0 0 16px 0; font-size: 20px; font-weight: 700; color: var(--text); padding-bottom: 8px; border-bottom: 2px solid var(--primary); display: inline-block;}
.alerts { display: grid; gap: 12px; }
.alert { padding: 14px 18px; border-radius: 12px; border: 1px solid transparent; display: flex; align-items: center; gap: 10px; }
.alert::before { font-size: 20px; }
.alert.critical { border-color:#fecaca; background:#fff1f2; color:#b91c1c; }
.alert.critical::before { content: 'ğŸš¨'; }
.alert.warn { border-color:#fde68a; background:#fffbeb; color: #b45309; }
.alert.warn::before { content: 'âš ï¸'; }
.alert.ok { border-color:#bbf7d0; background:#f0fdf4; color: #15803d; }
.alert.ok::before { content: 'âœ…'; }
@media (prefers-color-scheme: dark) {
  .alert.critical { background:#2d1516; border-color:#7f1d1d; color:#fca5a5; }
  .alert.warn { background:#2d230d; border-color:#7c5800; color:#fde047; }
  .alert.ok { background:#112a1a; border-color:#14532d; color:#86efac; }
}
.grade { display:inline-block; padding: 8px 14px; border-radius:999px; border:1px solid rgba(255,255,255,.5); font-weight:700; background: rgba(255,255,255,.2); backdrop-filter: blur(5px); }
.footer-note { color: var(--muted); font-size: 13px; text-align:center; padding: 24px; background: var(--bg); }
.badge { display:inline-block; padding:4px 10px; border-radius: 8px; background: var(--badge-bg); color: var(--badge-text); border:1px solid var(--badge-border); font-size:13px; font-weight: 500;}
.report-section p { margin: 0 0 12px 0; color: var(--text-weak); }
.report-section li { margin-bottom: 8px; color: var(--text-weak); }
.report-section a { color: var(--accent); text-decoration: none; font-weight: 500; }
.report-section a:hover { text-decoration: underline; }
.report-section strong { font-weight: 700; color: var(--text); }

/* ğŸ”¥ FIXED: ì½”ë“œ ë¸”ë¡ ë° ê¸´ í…ìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ */
.report-section pre, .translation-content pre {
    white-space: pre-wrap !important;
    word-wrap: break-word !important;
    overflow-wrap: break-word !important;
    background: var(--badge-bg);
    padding: 1rem;
    border-radius: 8px;
    border: 1px solid var(--border);
}

.report-section code, .translation-content code {
    white-space: pre-wrap !important;
    word-wrap: break-word !important;
    overflow-wrap: break-word !important;
    font-family: monospace;
    font-size: 0.9em;
    padding: 2px 6px;
    border-radius: 4px;
    background: var(--badge-bg);
}

/* ğŸ”¥ FIXED: í…Œì´ë¸” ë°˜ì‘í˜• ì²˜ë¦¬ (ë² íŠ¸ë‚¨ì–´ ì§€ì› ê°•í™”) */
.report-section table, .translation-content table {
    width: 100%;
    border-collapse: collapse;
    margin: 1rem 0;
    background: white;
    border-radius: 8px;
    overflow: hidden;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}
.report-section table { table-layout: fixed; }
.translation-content table { table-layout: auto; }
.translation-content .table-wrapper { 
    overflow-x: auto; 
    margin: 20px 0;
    border-radius: 8px;
    box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}
.report-section th, .report-section td, .translation-content th, .translation-content td {
    border: 1px solid var(--border);
    padding: 12px 16px;
    word-wrap: break-word;
    overflow-wrap: break-word;
    vertical-align: top;
}
.report-section th {
    background-color: var(--badge-bg);
    font-weight: 600;
}
.translation-content th {
    background: var(--primary);
    color: white;
    font-weight: 600;
    border-bottom: 2px solid var(--primary-dark);
}
.translation-content td {
    border-bottom: 1px solid var(--border);
}
.translation-content tr:nth-child(even) {
    background: var(--bg-light);
}
.translation-content tr:hover {
    background: var(--bg-hover);
}

/* ğŸ”¥ FIXED: ê¸´ ë‹¨ì–´ ê°•ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë ˆì´ì•„ì›ƒ ê¹¨ì§ ë°©ì§€ */
.report-section *, .translation-content * {
    word-break: break-word;
    overflow-wrap: break-word;
}


/* ë²ˆì—­ ê²°ê³¼ ì „ìš© ìŠ¤íƒ€ì¼ ì¶”ê°€ */
    .translation-content {
        background: var(--card-bg);
        border: 1px solid var(--border);
        border-radius: 16px;
        padding: 24px;
        margin: 8px 0;
        box-shadow: 0 4px 20px var(--shadow);
        line-height: 1.7;
        /* ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ì ì§€ì›ì„ ìœ„í•œ í°íŠ¸ ì„¤ì • */
        font-family: 'Noto Sans', 'Noto Sans KR', 'Noto Sans JP', 'Noto Sans SC', sans-serif;
    }
.translation-content h1, .translation-content h2, .translation-content h3 {
    color: var(--primary);
    margin-top: 24px;
    margin-bottom: 16px;
}
.translation-content h1 {
    font-size: 24px;
    font-weight: 700;
    border-bottom: 2px solid var(--primary);
    padding-bottom: 8px;
}
.translation-content h2 {
    font-size: 20px;
    font-weight: 600;
}
.translation-content h3 {
    font-size: 18px;
    font-weight: 500;
}
.translation-content p {
    margin-bottom: 12px;
    color: var(--text-weak);
}
.translation-content ul, .translation-content ol {
    margin: 16px 0;
    padding-left: 24px;
}
.translation-content li {
    margin-bottom: 8px;
    color: var(--text-weak);
}
.translation-content strong {
    color: var(--text);
    font-weight: 600;
}
.translation-content blockquote {
    border-left: 4px solid var(--primary);
    padding-left: 16px;
    margin: 16px 0;
    font-style: italic;
    color: var(--muted);
}
"""

EMBED_CSS = DEFAULT_EMBED_CSS

# ğŸ”¥ FIXED: í–¥ìƒëœ ë§ˆí¬ë‹¤ìš´ -> HTML ë³€í™˜ í•¨ìˆ˜ (ë² íŠ¸ë‚¨ì–´ ë° í…Œì´ë¸” ì§€ì› ê°•í™”)
def md_to_html(md_text: str) -> str:
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë² íŠ¸ë‚¨ì–´ ë° í…Œì´ë¸” ì²˜ë¦¬ë¥¼ ê°•í™”í–ˆìŠµë‹ˆë‹¤."""
    if not md_text:
        return ""
    
    if MARKDOWN_AVAILABLE:
        try:
            # markdown2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ìš°ì„  ì‚¬ìš© (í…Œì´ë¸” ì§€ì› ê°•í™”)
            import markdown2
            return markdown2.markdown(
                md_text, 
                extras=[
                    "fenced-code-blocks", 
                    "tables", 
                    "break-on-newline", 
                    "spoiler",
                    "strike",
                    "target-blank-links",
                    "cuddled-lists",
                    "footnotes"
                ]
            )
        except:
            try:
                # markdown ë¼ì´ë¸ŒëŸ¬ë¦¬ ëŒ€ì•ˆ ì‚¬ìš©
                import markdown
                return markdown.markdown(
                    md_text,
                    extensions=['codehilite', 'tables', 'fenced_code', 'nl2br', 'attr_list']
                )
            except:
                pass
    
    # í´ë°±: ê¸°ë³¸ ë§ˆí¬ë‹¤ìš´ íŒŒì‹± (ë² íŠ¸ë‚¨ì–´ ë° í…Œì´ë¸” ì§€ì› ê°•í™”)
    # ì „ê° ê¸°í˜¸ ì •ê·œí™”: ï½œ(U+FF5C), ï¼(U+FF0D) ë“±ì„ ASCIIë¡œ ë³€í™˜í•´ í…Œì´ë¸”/ìˆ˜í‰ì„  ì¸ì‹ ê°œì„ 
    html = (
        md_text
        .replace('ï½œ', '|')
        .replace('ï¿¨', '|')
        .replace('ï¼', '-')
        .replace('ï¹£', '-')
        .replace('â€”', '-')
        .replace('â€“', '-')
    )
    
    # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ì ë³´ì¡´
    html = preserve_vietnamese_chars(html)
    
    # ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ (``` êµ¬ë¬¸)
    html = re.sub(r'```(\w+)?\n(.*?)\n```', r'<pre><code>\2</code></pre>', html, flags=re.DOTALL)
    html = re.sub(r'```\n(.*?)\n```', r'<pre><code>\1</code></pre>', html, flags=re.DOTALL)
    
    # ì¸ë¼ì¸ ì½”ë“œ ì²˜ë¦¬
    html = re.sub(r'`([^`]+)`', r'<code>\1</code>', html)
    
    # í—¤ë”© ë³€í™˜ (ê°œì„ ëœ íŒ¨í„´)
    html = re.sub(r'^###\s*(.*)$', r'<h3>\1</h3>', html, flags=re.MULTILINE)
    html = re.sub(r'^##\s*(.*)$', r'<h2>\1</h2>', html, flags=re.MULTILINE)
    html = re.sub(r'^#\s*(.*)$', r'<h1>\1</h1>', html, flags=re.MULTILINE)
    
    # ê°•ì¡° í…ìŠ¤íŠ¸ ì²˜ë¦¬
    html = re.sub(r'\*\*\*(.*?)\*\*\*', r'<strong><em>\1</em></strong>', html)  # ë³¼ë“œ+ì´íƒ¤ë¦­
    html = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html)  # ë³¼ë“œ
    html = re.sub(r'\*(.*?)\*', r'<em>\1</em>', html)  # ì´íƒ¤ë¦­
    
    # ì·¨ì†Œì„ 
    html = re.sub(r'~~(.*?)~~', r'<del>\1</del>', html)
    
    # ë§í¬ ì²˜ë¦¬
    html = re.sub(r'\[([^\]]+)\]\(([^)]+)\)', r'<a href="\2" target="_blank">\1</a>', html)
    
    # í…Œì´ë¸” ì²˜ë¦¬ (ê°•í™”ëœ ë²„ì „)
    html = process_markdown_tables(html)
    
    # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
    lines = html.split('\n')
    in_ul = False
    in_ol = False
    result_lines = []
    
    for line in lines:
        stripped = line.strip()
        
        # ìˆœì„œ ìˆëŠ” ë¦¬ìŠ¤íŠ¸
        if re.match(r'^\d+\.\s+', stripped):
            if not in_ol:
                if in_ul:
                    result_lines.append('</ul>')
                    in_ul = False
                result_lines.append('<ol>')
                in_ol = True
            content = re.sub(r'^\d+\.\s+', '', stripped)
            result_lines.append(f'<li>{content}</li>')
        # ìˆœì„œ ì—†ëŠ” ë¦¬ìŠ¤íŠ¸
        elif re.match(r'^[-*+]\s+', stripped):
            if not in_ul:
                if in_ol:
                    result_lines.append('</ol>')
                    in_ol = False
                result_lines.append('<ul>')
                in_ul = True
            content = re.sub(r'^[-*+]\s+', '', stripped)
            result_lines.append(f'<li>{content}</li>')
        else:
            # ë¦¬ìŠ¤íŠ¸ ì¢…ë£Œ
            if in_ul:
                result_lines.append('</ul>')
                in_ul = False
            if in_ol:
                result_lines.append('</ol>')
                in_ol = False
            result_lines.append(line)
    
    # ë‚¨ì€ ë¦¬ìŠ¤íŠ¸ íƒœê·¸ ì •ë¦¬
    if in_ul:
        result_lines.append('</ul>')
    if in_ol:
        result_lines.append('</ol>')
    
    html = '\n'.join(result_lines)
    
    # ë¸”ë¡ì¿¼íŠ¸ ì²˜ë¦¬
    html = re.sub(r'^>\s*(.*)$', r'<blockquote>\1</blockquote>', html, flags=re.MULTILINE)
    
    # ìˆ˜í‰ì„  ì²˜ë¦¬
    html = re.sub(r'^---+$', r'<hr>', html, flags=re.MULTILINE)
    html = re.sub(r'^\*\*\*+$', r'<hr>', html, flags=re.MULTILINE)
    
    # ë‹¨ë½ ì²˜ë¦¬ (ê°œì„ ëœ ë²„ì „)
    paragraphs = re.split(r'\n\s*\n', html)
    processed_paragraphs = []
    
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        
        # HTML íƒœê·¸ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ë‹¨ë½ íƒœê·¸ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        if re.match(r'^<(?:h[1-6]|ul|ol|li|blockquote|pre|hr|div|table)', para, re.IGNORECASE):
            processed_paragraphs.append(para)
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” p íƒœê·¸ë¡œ ê°ì‹¸ê¸°
            para = para.replace('\n', '<br>')
            processed_paragraphs.append(f'<p>{para}</p>')
    
    html = '\n\n'.join(processed_paragraphs)
    
    return html

def preprocess_markdown_for_translation(text: str) -> str:
    """
    ë²ˆì—­ëœ í…ìŠ¤íŠ¸ì˜ ë§ˆí¬ë‹¤ìš´ì„ ì „ì²˜ë¦¬í•˜ì—¬ í…Œì´ë¸” ê¹¨ì§ í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    if not text:
        return text
    
    # í…Œì´ë¸” êµ¬ì¡° ë³´ì¡´ì„ ìœ„í•œ ì „ì²˜ë¦¬
    lines = text.split('\n')
    processed_lines = []
    in_table = False
    table_buffer = []
    
    for line in lines:
        stripped = line.strip()
        
        # í…Œì´ë¸” ì‹œì‘ ê°ì§€ (íŒŒì´í”„ | í¬í•¨)
        if '|' in stripped and not stripped.startswith('http'):
            if not in_table:
                in_table = True
                table_buffer = []
            table_buffer.append(line)
        # í…Œì´ë¸” êµ¬ë¶„ì„  ê°ì§€ (--- ë˜ëŠ” ===)
        elif in_table and (stripped.startswith('|') and ('---' in stripped or '===' in stripped)):
            table_buffer.append(line)
        # í…Œì´ë¸” ì¢…ë£Œ ê°ì§€ (ë¹ˆ ì¤„ ë˜ëŠ” íŒŒì´í”„ê°€ ì—†ëŠ” ì¤„)
        elif in_table and (not stripped or '|' not in stripped):
            # í…Œì´ë¸” ë²„í¼ ì²˜ë¦¬
            if table_buffer:
                processed_lines.extend(process_table_markdown(table_buffer))
                table_buffer = []
            in_table = False
            processed_lines.append(line)
        # í…Œì´ë¸” ë‚´ë¶€ ë¼ì¸
        elif in_table:
            table_buffer.append(line)
        # ì¼ë°˜ í…ìŠ¤íŠ¸
        else:
            processed_lines.append(line)
    
    # ë§ˆì§€ë§‰ í…Œì´ë¸” ì²˜ë¦¬
    if table_buffer:
        processed_lines.extend(process_table_markdown(table_buffer))
    
    return '\n'.join(processed_lines)

def process_table_markdown(table_lines: list) -> list:
    """
    í…Œì´ë¸” ë§ˆí¬ë‹¤ìš´ì„ ì²˜ë¦¬í•˜ì—¬ ê¹¨ì§ í˜„ìƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    if not table_lines:
        return []
    
    processed_lines = []
    
    for i, line in enumerate(table_lines):
        if '|' in line:
            # í…Œì´ë¸” ì…€ ë‚´ìš© ì •ë¦¬
            cells = line.split('|')
            cleaned_cells = []
            
            for cell in cells:
                # ì…€ ë‚´ìš© ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
                cleaned_cell = cell.strip()
                if cleaned_cell:
                    # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ì ë³´ì¡´
                    cleaned_cell = preserve_vietnamese_chars(cleaned_cell)
                    cleaned_cells.append(cleaned_cell)
                else:
                    cleaned_cells.append(' ')
            
            # í…Œì´ë¸” ë¼ì¸ ì¬êµ¬ì„±
            processed_line = '|' + '|'.join(cleaned_cells) + '|'
            processed_lines.append(processed_line)
        else:
            processed_lines.append(line)
    
    return processed_lines

def process_markdown_tables(html: str) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì„ HTML í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    lines = html.split('\n')
    result_lines = []
    i = 0
    
    while i < len(lines):
        line = lines[i].strip()
        
        # í…Œì´ë¸” ì‹œì‘ ê°ì§€ (íŒŒì´í”„ | í¬í•¨)
        if '|' in line and not line.startswith('http'):
            table_lines = []
            header_line = line
            
            # í—¤ë” ë¼ì¸ ì¶”ê°€
            table_lines.append(header_line)
            i += 1
            
            # êµ¬ë¶„ì„  í™•ì¸
            if i < len(lines) and '|' in lines[i] and ('---' in lines[i] or '===' in lines[i]):
                separator_line = lines[i]
                table_lines.append(separator_line)
                i += 1
            
            # í…Œì´ë¸” ë³¸ë¬¸ ìˆ˜ì§‘
            while i < len(lines) and '|' in lines[i]:
                table_lines.append(lines[i])
                i += 1
            
            # í…Œì´ë¸”ì„ HTMLë¡œ ë³€í™˜
            html_table = convert_table_to_html(table_lines)
            result_lines.append(html_table)
        else:
            result_lines.append(lines[i])
            i += 1
    
    return '\n'.join(result_lines)

def convert_table_to_html(table_lines: list) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” ë¼ì¸ì„ HTML í…Œì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not table_lines:
        return ""
    
    html_parts = ['<div class="table-wrapper">', '<table>']
    
    for i, line in enumerate(table_lines):
        if '|' in line:
            # êµ¬ë¶„ì„  ë¼ì¸ì€ ê±´ë„ˆë›°ê¸°
            if '---' in line or '===' in line:
                continue
            
            # í…Œì´ë¸” ì…€ ë¶„ë¦¬ ë° ì •ë¦¬
            cells = [cell.strip() for cell in line.split('|')]
            
            # ë¹ˆ ì…€ ì²˜ë¦¬
            if cells and not cells[0]:  # ì²« ë²ˆì§¸ ë¹ˆ ì…€ ì œê±°
                cells = cells[1:]
            if cells and not cells[-1]:  # ë§ˆì§€ë§‰ ë¹ˆ ì…€ ì œê±°
                cells = cells[:-1]
            
            if cells:
                if i == 0:  # í—¤ë” ë¼ì¸
                    html_parts.append('<thead>')
                    html_parts.append('<tr>')
                    for cell in cells:
                        html_parts.append(f'<th>{cell}</th>')
                    html_parts.append('</tr>')
                    html_parts.append('</thead>')
                    html_parts.append('<tbody>')
                else:  # ë³¸ë¬¸ ë¼ì¸
                    html_parts.append('<tr>')
                    for cell in cells:
                        html_parts.append(f'<td>{cell}</td>')
                    html_parts.append('</tr>')
    
    html_parts.append('</tbody>')
    html_parts.append('</table>')
    html_parts.append('</div>')
    
    return '\n'.join(html_parts)

def preserve_vietnamese_chars(text: str) -> str:
    """
    ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
    """
    # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ì ë§¤í•‘
    vietnamese_chars = {
        'Ã ': 'Ã ', 'Ã¡': 'Ã¡', 'áº¡': 'áº¡', 'áº£': 'áº£', 'Ã£': 'Ã£',
        'Ã¢': 'Ã¢', 'áº§': 'áº§', 'áº¥': 'áº¥', 'áº­': 'áº­', 'áº©': 'áº©', 'áº«': 'áº«',
        'Äƒ': 'Äƒ', 'áº±': 'áº±', 'áº¯': 'áº¯', 'áº·': 'áº·', 'áº³': 'áº³', 'áºµ': 'áºµ',
        'Ã¨': 'Ã¨', 'Ã©': 'Ã©', 'áº¹': 'áº¹', 'áº»': 'áº»', 'áº½': 'áº½',
        'Ãª': 'Ãª', 'á»': 'á»', 'áº¿': 'áº¿', 'á»‡': 'á»‡', 'á»ƒ': 'á»ƒ', 'á»…': 'á»…',
        'Ã¬': 'Ã¬', 'Ã­': 'Ã­', 'á»‹': 'á»‹', 'á»‰': 'á»‰', 'Ä©': 'Ä©',
        'Ã²': 'Ã²', 'Ã³': 'Ã³', 'á»': 'á»', 'á»': 'á»', 'Ãµ': 'Ãµ',
        'Ã´': 'Ã´', 'á»“': 'á»“', 'á»‘': 'á»‘', 'á»™': 'á»™', 'á»•': 'á»•', 'á»—': 'á»—',
        'Æ¡': 'Æ¡', 'á»': 'á»', 'á»›': 'á»›', 'á»£': 'á»£', 'á»Ÿ': 'á»Ÿ', 'á»¡': 'á»¡',
        'Ã¹': 'Ã¹', 'Ãº': 'Ãº', 'á»¥': 'á»¥', 'á»§': 'á»§', 'Å©': 'Å©',
        'Æ°': 'Æ°', 'á»«': 'á»«', 'á»©': 'á»©', 'á»±': 'á»±', 'á»­': 'á»­', 'á»¯': 'á»¯',
        'á»³': 'á»³', 'Ã½': 'Ã½', 'á»µ': 'á»µ', 'á»·': 'á»·', 'á»¹': 'á»¹',
        'Ä‘': 'Ä‘'
    }
    
    for original, preserved in vietnamese_chars.items():
        text = text.replace(original, preserved)
    
    return text

def create_translated_html(translated_text: str, title: str = "ë²ˆì—­ëœ ë‚´ìš©") -> str:
    """ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ ì˜ˆìœ HTMLë¡œ ë³€í™˜ (ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ê°œì„  + ë² íŠ¸ë‚¨ì–´ ì§€ì›)"""
    # íŠ¹ìˆ˜ë¬¸ì ë° ì´ëª¨ì§€ ì²˜ë¦¬ ê°œì„ 
    processed_text = translated_text
    
    # ìš°í¬ë¼ì´ë‚˜ì–´ ë²ˆì—­ ê²°ê³¼ì—ì„œ "0000" ê°™ì€ íŠ¹ìˆ˜ íŒ¨í„´ ì²˜ë¦¬
    if "0000" in processed_text and any(char in processed_text for char in "Ğ°Ğ±Ğ²Ğ³Ğ´ĞµÑ‘Ğ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑ"):
        # ìš°í¬ë¼ì´ë‚˜ì–´ í…ìŠ¤íŠ¸ë¡œ ì¸ì‹í•˜ì—¬ íŠ¹ìˆ˜ ì²˜ë¦¬
        processed_text = processed_text.replace("0000", "Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ")  # ì‹¤ì œ ìš°í¬ë¼ì´ë‚˜ì–´ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´
    
    # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ (ë§ˆí¬ë‹¤ìš´ ì¸ì‹ ê°œì„ )
    if any(char in processed_text for char in "Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘"):
        # ë² íŠ¸ë‚¨ì–´ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        processed_text = processed_text.replace("Ã ", "Ã ").replace("Ã¡", "Ã¡").replace("áº¡", "áº¡")
        processed_text = processed_text.replace("áº£", "áº£").replace("Ã£", "Ã£").replace("Ã¢", "Ã¢")
        processed_text = processed_text.replace("áº§", "áº§").replace("áº¥", "áº¥").replace("áº­", "áº­")
        processed_text = processed_text.replace("áº©", "áº©").replace("áº«", "áº«").replace("Äƒ", "Äƒ")
        processed_text = processed_text.replace("áº±", "áº±").replace("áº¯", "áº¯").replace("áº·", "áº·")
        processed_text = processed_text.replace("áº³", "áº³").replace("áºµ", "áºµ").replace("Ã¨", "Ã¨")
        processed_text = processed_text.replace("Ã©", "Ã©").replace("áº¹", "áº¹").replace("áº»", "áº»")
        processed_text = processed_text.replace("áº½", "áº½").replace("Ãª", "Ãª").replace("á»", "á»")
        processed_text = processed_text.replace("áº¿", "áº¿").replace("á»‡", "á»‡").replace("á»ƒ", "á»ƒ")
        processed_text = processed_text.replace("á»…", "á»…").replace("Ã¬", "Ã¬").replace("Ã­", "Ã­")
        processed_text = processed_text.replace("á»‹", "á»‹").replace("á»‰", "á»‰").replace("Ä©", "Ä©")
        processed_text = processed_text.replace("Ã²", "Ã²").replace("Ã³", "Ã³").replace("á»", "á»")
        processed_text = processed_text.replace("á»", "á»").replace("Ãµ", "Ãµ").replace("Ã´", "Ã´")
        processed_text = processed_text.replace("á»“", "á»“").replace("á»‘", "á»‘").replace("á»™", "á»™")
        processed_text = processed_text.replace("á»•", "á»•").replace("á»—", "á»—").replace("Æ¡", "Æ¡")
        processed_text = processed_text.replace("á»", "á»").replace("á»›", "á»›").replace("á»£", "á»£")
        processed_text = processed_text.replace("á»Ÿ", "á»Ÿ").replace("á»¡", "á»¡").replace("Ã¹", "Ã¹")
        processed_text = processed_text.replace("Ãº", "Ãº").replace("á»¥", "á»¥").replace("á»§", "á»§")
        processed_text = processed_text.replace("Å©", "Å©").replace("Æ°", "Æ°").replace("á»«", "á»«")
        processed_text = processed_text.replace("á»©", "á»©").replace("á»±", "á»±").replace("á»­", "á»­")
        processed_text = processed_text.replace("á»¯", "á»¯").replace("á»³", "á»³").replace("Ã½", "Ã½")
        processed_text = processed_text.replace("á»µ", "á»µ").replace("á»·", "á»·").replace("á»¹", "á»¹")
        processed_text = processed_text.replace("Ä‘", "Ä‘")
    
    # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì „ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (í…Œì´ë¸” ê¹¨ì§ ë°©ì§€)
    processed_text = preprocess_markdown_for_translation(processed_text)
    
    html_content = md_to_html(processed_text)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    
    return f"""
    <html>
      <head>
        {EMBED_HEAD}
        <style>{EMBED_CSS}</style>
      </head>
      <body>
        <div class="report-wrap">
          <div class="translation-content">
            <div style="border-bottom: 1px solid var(--border); padding-bottom: 16px; margin-bottom: 24px;">
              <h1 style="margin: 0; color: var(--primary);">ğŸŒ {title}</h1>
              <p style="margin: 8px 0 0 0; color: var(--muted); font-size: 14px;">ğŸ“… {timestamp}</p>
            </div>
            <div>{html_content}</div>
          </div>
        </div>
      </body>
    </html>
    """

def extract_landlord_name_robustly(contract_text: str) -> str:
    """ğŸ”¥ 3ë‹¨ê³„ì— ê±¸ì³ ì„ëŒ€ì¸ ì´ë¦„ì„ ì§‘ìš”í•˜ê²Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (Gradioìš© ìˆ˜ì •)"""
    llm = ChatUpstage()

    # --- 1ë‹¨ê³„: ì´ë¦„ë§Œ ì •í™•íˆ ì¶”ì¶œ ì‹œë„ ---
    prompt_step1 = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ê³„ì•½ì„œ í…ìŠ¤íŠ¸ì—ì„œ 'ì„ëŒ€ì¸' ë˜ëŠ” 'ì§‘ì£¼ì¸'ì˜ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì¤˜. "
        "ë‹¤ë¥¸ ë§ì€ ëª¨ë‘ ì œì™¸í•˜ê³  ì´ë¦„ë§Œ ë§í•´ì¤˜. (ì˜ˆ: í™ê¸¸ë™). "
        "ë§Œì•½ ì´ë¦„ì´ ì—†ìœ¼ë©´ 'ì—†ìŒ'ì´ë¼ê³  ë§í•´ì¤˜. í…ìŠ¤íŠ¸: {contract}"
    )
    chain_step1 = prompt_step1 | llm | StrOutputParser()
    name_step1 = chain_step1.invoke({"contract": contract_text}).strip()

    # 1ë‹¨ê³„ ê²€ì¦: 2~5ê¸€ìì˜ í•œê¸€ ì´ë¦„ì¸ì§€ í™•ì¸
    if re.fullmatch(r'[ê°€-í£]{2,5}', name_step1.replace(" ", "")):
        print(f"  [ì„ëŒ€ì¸ ê²€ì‚¬] 1ë‹¨ê³„ ì„±ê³µ: '{name_step1}' ì¶”ì¶œ")
        return name_step1.replace(" ", "")

    # --- 2ë‹¨ê³„: ì‹¤íŒ¨ ì‹œ, ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ì¶œ í›„ íŒŒì´ì¬ìœ¼ë¡œ ì´ë¦„ ì°¾ê¸° ---
    print("  [ì„ëŒ€ì¸ ê²€ì‚¬] 1ë‹¨ê³„ ì‹¤íŒ¨, 2ë‹¨ê³„ ì‹œë„ ì¤‘...")
    prompt_step2 = ChatPromptTemplate.from_template(
        "ë‹¤ìŒ ê³„ì•½ì„œ í…ìŠ¤íŠ¸ì—ì„œ 'ì„ëŒ€ì¸' ë˜ëŠ” 'ì§‘ì£¼ì¸'ì˜ ì´ë¦„ì´ í¬í•¨ëœ ë¼ì¸ ë˜ëŠ” ë¬¸ì¥ ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì•Œë ¤ì¤˜. "
        "í…ìŠ¤íŠ¸: {contract}"
    )
    chain_step2 = prompt_step2 | llm | StrOutputParser()
    sentence = chain_step2.invoke({"contract": contract_text})
    
    # 2ë‹¨ê³„ ê²€ì¦: ë¬¸ì¥ì—ì„œ 2~5ê¸€ì í•œê¸€ íŒ¨í„´ ì°¾ê¸°
    match = re.search(r'[ê°€-í£]{2,5}', sentence)
    if match:
        name_step2 = match.group(0)
        print(f"  [ì„ëŒ€ì¸ ê²€ì‚¬] 2ë‹¨ê³„ ì„±ê³µ: '{name_step2}' ì¶”ì¶œ")
        return name_step2

    # --- 3ë‹¨ê³„: AI ì¶”ì¶œ ì‹¤íŒ¨ ì•Œë¦¼ ---
    # Gradio í™˜ê²½ì—ì„œëŠ” CLIì²˜ëŸ¼ ì‚¬ìš©ì ì…ë ¥(input)ì„ ë°›ì„ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
    # ìë™ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŒì„ ì•Œë¦¬ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
    print("  [ì„ëŒ€ì¸ ê²€ì‚¬] 2ë‹¨ê³„ ì‹¤íŒ¨. ìë™ ì´ë¦„ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    return "ì´ë¦„ ìë™ ì¶”ì¶œ ì‹¤íŒ¨"

# ### MODIFIED FUNCTION ###: get_multilingual_fontì— lang_codeë¥¼ ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •
def create_clean_report_image(report_text: str, report_type: str = "report", lang_code: str = 'KO') -> Image.Image:
    """ê¹”ë”í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¦¬í¬íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ë‹¤êµ­ì–´ ì§€ì› + ì´ëª¨ì§€ ì§€ì›)"""
    width = 1200
    margin = 50
    line_height = 28
    
    # ë‹¤êµ­ì–´ í°íŠ¸ ì„¤ì • (ì–¸ì–´ ì½”ë“œ ì „ë‹¬)
    title_font = get_multilingual_font(28, bold=True, lang_code=lang_code)
    heading_font = get_multilingual_font(20, bold=True, lang_code=lang_code) 
    text_font = get_multilingual_font(16, bold=False, lang_code=lang_code)
    small_font = get_multilingual_font(14, bold=False, lang_code=lang_code)
    
    # ì´ëª¨ì§€ í°íŠ¸ ì„¤ì •
    emoji_font = get_emoji_font(16)
    
    # í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì•ˆì „ì¥ì¹˜
    if not title_font or not heading_font or not text_font or not small_font:
        print("âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨ - í…ìŠ¤íŠ¸ ë Œë”ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±
        img = Image.new('RGB', (width, 600), '#ffffff')
        draw = ImageDraw.Draw(img)
        error_font = ImageFont.load_default()
        draw.text((margin, margin), "A required font could not be loaded.\nCannot render the report image.", fill='#dc2626', font=error_font)
        return img
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë†’ì´ ê³„ì‚°
    lines = []
    current_y = margin + 60
    
    # ì œëª© ì¶”ê°€ (ì´ëª¨ì§€ í¬í•¨)
    if "translation" in report_type.lower() or "ë²ˆì—­" in report_type:
        if lang_code == 'EN':
            title = "ğŸŒ Translation Result"
        elif lang_code == 'JA':
            title = "ğŸŒ ç¿»è¨³çµæœ"
        elif lang_code == 'ZH':
            title = "ğŸŒ ç¿»è¯‘ç»“æœ"
        # ìš°í¬ë¼ì´ë‚˜ì–´, ë² íŠ¸ë‚¨ì–´ ì œëª© ì¶”ê°€
        elif lang_code == 'UK':
            title = "ğŸŒ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ"
        elif lang_code == 'VI':
            title = "ğŸŒ Káº¿t quáº£ dá»‹ch"
        else:
            title = "ğŸŒ ë²ˆì—­ ê²°ê³¼"
    elif "analysis" in report_type or "ë¶„ì„" in report_type:
        title = "AI ë¶€ë™ì‚° ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸"
    else:
        title = "AI ìƒë‹´ ë‹µë³€"
    
    lines.append(('title', title, current_y))
    current_y += 50
    
    # ë‚ ì§œ ì¶”ê°€ (ì´ëª¨ì§€ ì œê±°í•˜ì—¬ tofu ë°©ì§€)
    now = datetime.now()
    if lang_code == 'KO':
        date_str = f"ìƒì„±ì¼ì‹œ: {now.strftime('%Y')}ë…„ {now.strftime('%m')}ì›” {now.strftime('%d')}ì¼ {now.strftime('%H')}ì‹œ {now.strftime('%M')}ë¶„"
    elif lang_code == 'EN':
        date_str = f"Generated: {now.strftime('%Y-%m-%d %H:%M')}"
    elif lang_code == 'JA':
        date_str = f"ç”Ÿæˆæ—¥æ™‚: {now.strftime('%Y')}å¹´{now.strftime('%m')}æœˆ{now.strftime('%d')}æ—¥ {now.strftime('%H')}æ™‚{now.strftime('%M')}åˆ†"
    elif lang_code == 'ZH':
        date_str = f"ç”Ÿæˆæ—¶é—´: {now.strftime('%Y')}å¹´{now.strftime('%m')}æœˆ{now.strftime('%d')}æ—¥ {now.strftime('%H')}æ—¶{now.strftime('%M')}åˆ†"
    elif lang_code == 'UK':
        date_str = f"Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¾: {now.strftime('%Y-%m-%d %H:%M')}"
    elif lang_code == 'VI':
        date_str = f"ÄÆ°á»£c táº¡o: {now.strftime('%Y-%m-%d %H:%M')}"
    else: # í˜¹ì‹œ ëª¨ë¥¼ ì˜ˆì™¸ ì²˜ë¦¬
        date_str = f"Generated: {now.strftime('%Y-%m-%d %H:%M')}"

    lines.append(('date', date_str, current_y))
    current_y += 40
    
    # êµ¬ë¶„ì„ 
    lines.append(('divider', '', current_y))
    current_y += 30
    
    # ë³¸ë¬¸ ì²˜ë¦¬
    for line in report_text.split('\n'):
        line = line.strip()
        if not line:
            current_y += 15
            continue
            
        # í—¤ë”© ì²˜ë¦¬ (# ì œê±°)
        if line.startswith('# '):
            text = line[2:].strip()
            lines.append(('h1', text, current_y))
            current_y += 45
        elif line.startswith('## '):
            text = line[3:].strip()
            lines.append(('h2', text, current_y))
            current_y += 35
        elif line.startswith('### '):
            text = line[4:].strip()
            lines.append(('h3', text, current_y))
            current_y += 30
        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        elif line.startswith('- '):
            text = line[2:].strip()
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ìë™ ì¤„ë°”ê¿ˆ (ì–¸ì–´ë³„ ë„ˆë¹„ ì¡°ì •)
            if lang_code in ['UK', 'ZH', 'JA']:
                # í‚¤ë¦´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ëŠ” ë” ì¢ì€ ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆ
                wrapped = textwrap.fill(text, width=50)
            else:
                # í•œêµ­ì–´, ì˜ì–´ ë“±ì€ ê¸°ì¡´ ë„ˆë¹„
                wrapped = textwrap.fill(text, width=70)
            for wrapped_line in wrapped.split('\n'):
                lines.append(('bullet', wrapped_line, current_y))
                current_y += line_height
        # ë³¼ë“œ ì²˜ë¦¬ (** ì œê±°)
        elif line.startswith('**') and line.endswith('**'):
            text = line[2:-2].strip()
            lines.append(('bold', text, current_y))
            current_y += line_height
        # êµ¬ë¶„ì„ 
        elif '---' in line:
            lines.append(('divider', '', current_y))
            current_y += 20
        # ì¼ë°˜ í…ìŠ¤íŠ¸
        else:
            # ê¸´ ì¤„ ìë™ ì¤„ë°”ê¿ˆ (ì–¸ì–´ë³„ ë„ˆë¹„ ì¡°ì •)
            if lang_code in ['UK', 'ZH', 'JA']:
                # í‚¤ë¦´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ëŠ” ë” ì¢ì€ ë„ˆë¹„ë¡œ ì¤„ë°”ê¿ˆ
                wrapped = textwrap.fill(line, width=55)
            else:
                # í•œêµ­ì–´, ì˜ì–´ ë“±ì€ ê¸°ì¡´ ë„ˆë¹„
                wrapped = textwrap.fill(line, width=75)
            for wrapped_line in wrapped.split('\n'):
                lines.append(('text', wrapped_line, current_y))
                current_y += line_height
    
    # í‘¸í„° ê³µê°„
    current_y += 30
    footer_text = "ë³¸ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©° ë²•ì  íš¨ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì • ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
    # lang_code ê°’ ì²´ê³„(KO/EN/JA/ZH/UK/VI)ì— ë§ì¶° í˜„ì§€í™” (ì´ëª¨ì§€ ì œê±°í•˜ì—¬ tofu ë°©ì§€)
    if lang_code == 'EN':
        footer_text = "This analysis is for reference only and has no legal effect. Please consult with experts before making important decisions."
    elif lang_code == 'JA':
        footer_text = "ã“ã®åˆ†æã¯å‚è€ƒç”¨ã§ã‚ã‚Šã€æ³•çš„åŠ¹åŠ›ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚é‡è¦ãªæ±ºå®šã®å‰ã«å¿…ãšå°‚é–€å®¶ã«ã”ç›¸è«‡ãã ã•ã„ã€‚"
    elif lang_code == 'ZH':
        footer_text = "æœ¬åˆ†æä»…ä¾›å‚è€ƒï¼Œä¸å…·æœ‰æ³•å¾‹æ•ˆåŠ›ã€‚åœ¨åšå‡ºé‡è¦å†³å®šä¹‹å‰ï¼Œè¯·åŠ¡å¿…å’¨è¯¢ä¸“å®¶ã€‚"
    elif lang_code == 'UK':
        footer_text = "Ğ¦ĞµĞ¹ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ Ğ»Ğ¸ÑˆĞµ Ğ´Ğ»Ñ Ğ¾Ğ·Ğ½Ğ°Ğ¹Ğ¾Ğ¼Ğ»ĞµĞ½Ğ½Ñ Ñ‚Ğ° Ğ½Ğµ Ğ¼Ğ°Ñ” ÑÑ€Ğ¸Ğ´Ğ¸Ñ‡Ğ½Ğ¾Ñ— ÑĞ¸Ğ»Ğ¸. ĞŸÑ€Ğ¾ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚ÑƒĞ¹Ñ‚ĞµÑÑ Ğ· ĞµĞºÑĞ¿ĞµÑ€Ñ‚Ğ°Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ğ¿Ñ€Ğ¸Ğ¹Ğ½ÑÑ‚Ñ‚ÑĞ¼ Ğ²Ğ°Ğ¶Ğ»Ğ¸Ğ²Ğ¸Ñ… Ñ€Ñ–ÑˆĞµĞ½ÑŒ."
    elif lang_code == 'VI':
        footer_text = "PhÃ¢n tÃ­ch nÃ y chá»‰ mang tÃ­nh tham kháº£o vÃ  khÃ´ng cÃ³ hiá»‡u lá»±c phÃ¡p lÃ½. Vui lÃ²ng tham kháº£o Ã½ kiáº¿n chuyÃªn gia trÆ°á»›c khi Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh quan trá»ng."

    lines.append(('footer', footer_text, current_y))
    current_y += 50
    
    # ìµœì¢… ì´ë¯¸ì§€ í¬ê¸°
    total_height = current_y + margin
    
    # ì´ë¯¸ì§€ ìƒì„±
    img = Image.new('RGB', (width, total_height), '#ffffff')
    draw = ImageDraw.Draw(img)
    
    # í—¤ë” ë°°ê²½
    header_height = 120
    draw.rectangle([0, 0, width, header_height], fill='#10b981')
    
    # ë©”ì¸ ì»¨í…ì¸  ë°°ê²½ (í°ìƒ‰ ì¹´ë“œ)
    draw.rectangle([margin//2, header_height, width-margin//2, total_height-margin//2],
                   fill='#ffffff', outline='#e5e7eb', width=2)
    
    # í…ìŠ¤íŠ¸ ë Œë”ë§
    for line_type, text, y in lines:
        try:
            if line_type == 'title':
                # ì œëª© ì¤‘ì•™ ì •ë ¬ (ì´ëª¨ì§€ í¬í•¨)
                draw_text_with_emoji(draw, text, (width//2, 30), title_font, emoji_font, 'center', '#ffffff')
                
            elif line_type == 'date':
                # ë‚ ì§œ ìš°ì¸¡ ì •ë ¬
                bbox = draw.textbbox((0, 0), text, font=small_font)
                x = width - margin - (bbox[2] - bbox[0])
                draw.text((x, y), text, fill='#6b7280', font=small_font)
                
            elif line_type == 'divider':
                # êµ¬ë¶„ì„ 
                draw.line([margin, y, width-margin, y], fill='#e5e7eb', width=2)
                
            elif line_type == 'h1':
                h1_font = get_multilingual_font(22, bold=True, lang_code=lang_code) or title_font
                draw_text_with_emoji(draw, text, (margin, y), h1_font, emoji_font, 'left', '#10b981')
                # í—¤ë”© ë°‘ì¤„
                draw.line([margin, y+32, margin+300, y+32], fill='#10b981', width=3)
                
            elif line_type == 'h2':
                draw_text_with_emoji(draw, text, (margin, y), heading_font, emoji_font, 'left', '#047857')
                
            elif line_type == 'h3':
                h3_font = get_multilingual_font(18, bold=True, lang_code=lang_code) or heading_font
                draw_text_with_emoji(draw, text, (margin, y), h3_font, emoji_font, 'left', '#1f2937')
                
            elif line_type == 'bullet':
                # ë¶ˆë¦¿ í¬ì¸íŠ¸
                draw.text((margin, y), "â€¢", fill='#10b981', font=text_font)
                draw_text_with_emoji(draw, text, (margin + 20, y), text_font, emoji_font, 'left', '#374151')
                
            elif line_type == 'bold':
                bold_font = get_multilingual_font(16, bold=True, lang_code=lang_code) or text_font
                draw_text_with_emoji(draw, text, (margin, y), bold_font, emoji_font, 'left', '#dc2626')
                
            elif line_type == 'text':
                draw_text_with_emoji(draw, text, (margin, y), text_font, emoji_font, 'left', '#374151')
                
            elif line_type == 'footer':
                # í‘¸í„° í…ìŠ¤íŠ¸ ì¤‘ì•™ ì •ë ¬ ë° ìë™ ì¤„ë°”ê¿ˆ
                wrapped_footer = textwrap.wrap(text, width=100)
                footer_y = y
                for line in wrapped_footer:
                    bbox = draw.textbbox((0, 0), line, font=small_font)
                    x = (width - (bbox[2] - bbox[0])) // 2
                    draw.text((x, footer_y), line, fill='#6b7280', font=small_font)
                    footer_y += 20 # ì¤„ ê°„ê²©

        except Exception as e:
            # ê°œë³„ í…ìŠ¤íŠ¸ ë Œë”ë§ ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€
            print(f"âš ï¸ í…ìŠ¤íŠ¸ ë Œë”ë§ ì˜¤ë¥˜: {e}")
            continue
    
    return img


def render_report_html(file_name: str, rule_analysis: dict, ai_analysis: dict, title="ğŸ  AI ë¶€ë™ì‚° ê³„ì•½ì„œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸") -> str:
    score = rule_analysis.get("safety_score", -1)
    if score >= 80:
        grade_text = f"ë§¤ìš° ì•ˆì „ ({score}ì )"
    elif score >= 50:
        grade_text = f"ë³´í†µ ({score}ì )"
    elif score >= 0:
        grade_text = f"ìœ„í—˜ ({score}ì )"
    else:
        grade_text = "ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜"

    alerts = rule_analysis.get("alerts", [])
    alerts_html = []
    for a in alerts:
        cls = "alert"
        # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ alert ì¢…ë¥˜ ìë™ íŒë³„
        if any(keyword in a for keyword in ["ì¹˜ëª…", "ğŸš¨", "ë°˜ë“œì‹œ"]):
            cls += " critical"
        elif any(keyword in a for keyword in ["ìœ„í—˜", "ê²½ê³ ", "âš ï¸", "ì£¼ì˜"]):
            cls += " warn"
        else: # "ê¶Œì¥", "âœ…", "ğŸ’¡" ë“±
            cls += " ok"
        
        # ::before ì•„ì´ì½˜ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ì—ì„œ ì´ëª¨í‹°ì½˜ì€ ì œê±°
        clean_alert_text = re.sub(r'^[ğŸš¨âš ï¸âœ…ğŸ’¡ğŸ•µï¸]+', '', a).strip()
        alerts_html.append(f'<div class="{cls}">{md_to_html(clean_alert_text)}</div>')
        
    alerts_html = "\n".join(alerts_html) if alerts_html else '<div class="alert">í‘œì‹œí•  ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤.</div>'

    ai_block = md_to_html(ai_analysis.get("analysis", ""))
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    html = f"""
    <html>
      <head>
        {EMBED_HEAD}
        <style>{EMBED_CSS}</style>
      </head>
      <body>
        <div class="report-wrap">
          <article class="report-card">
            <header class="report-header">
              <h1>{title}</h1>
              <div class="meta">
                <span class="badge">ğŸ“„ {file_name}</span> 
                <span class="badge">ğŸ“… {timestamp}</span>
                <span class="badge">{grade_text}</span>
            </header>

            <section class="report-section">
              <h2>ê³„ì•½ì„œ ì•ˆì „ë„ ê²€ì‚¬</h2>
              <div class="alerts">{alerts_html}</div>
            </section>

            <section class="report-section">
              <h2>AI ì‹¬ì¸µ ë¶„ì„</h2>
              <div>{ai_block}</div>
            </section>

            <footer class="footer-note">
              ë³¸ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©° ë²•ì  íš¨ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì • ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
            </footer>
          </article>
        </div>
      </body>
    </html>
    """
    return html

def split_text_for_analysis(text: str, max_tokens: int = 3500) -> list:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ í† í° ì œí•œì— ë§ê²Œ ë¶„í• í•©ë‹ˆë‹¤.
    ëŒ€ëµì ì¸ í† í° ê³„ì‚°: 1 í† í° â‰ˆ 4 ë¬¸ì (í•œêµ­ì–´ ê¸°ì¤€)
    """
    if not text:
        return []
    
    # ëŒ€ëµì ì¸ í† í° ìˆ˜ ê³„ì‚° (í•œêµ­ì–´ ê¸°ì¤€)
    estimated_tokens = len(text) // 4
    
    if estimated_tokens <= max_tokens:
        return [text]
    
    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # í˜„ì¬ ì²­í¬ì— ë¬¸ë‹¨ì„ ì¶”ê°€í–ˆì„ ë•Œì˜ í† í° ìˆ˜ ê³„ì‚°
        test_chunk = current_chunk + paragraph + "\n\n"
        test_tokens = len(test_chunk) // 4
        
        if test_tokens <= max_tokens:
            current_chunk = test_chunk
        else:
            # í˜„ì¬ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            # ìƒˆ ì²­í¬ ì‹œì‘
            current_chunk = paragraph + "\n\n"
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def perform_ai_analysis(contract_text: str) -> dict:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì•½ì„œë¥¼ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤. (í† í° ì œí•œ ìë™ ì²˜ë¦¬)"""
    # RAG ê²€ìƒ‰ê¸°(RETRIEVER)ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not RETRIEVER:
        return {"analysis": "âš ï¸ AI ë¶„ì„ ì—”ì§„(RAG)ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ë‹¤ì‹œ ì‹œì‘í•˜ê±°ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."}
    
    try:
        # 1) Groundedness Check ê°ì²´ ìƒì„±
        groundedness_checker = UpstageGroundednessCheck()

        # 2) í”„ë¡¬í”„íŠ¸ ì •ì˜ (ê°„ì†Œí™”ëœ ë²„ì „)
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ í•œêµ­ ë¶€ë™ì‚° ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ [ê³„ì•½ì„œ]ë¥¼ ë¶„ì„í•˜ê³ , ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ëˆ„ë½ëœ ì¡°í•­ì´ ì—†ëŠ”ì§€ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ê³„ì•½ì„œ]
{contract}

[ë¶„ì„ ìš”ì²­]
1. **ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­**: ë…ì†Œ ì¡°í•­ì´ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ì‘ìš©í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ì§šì–´ì£¼ì„¸ìš”.
2. **ëˆ„ë½ëœ ì¤‘ìš” ì¡°í•­**: ì„ì°¨ì¸ ë³´í˜¸ë¥¼ ìœ„í•´ ì°¸ê³  ìë£Œì— ê·¼ê±°í•˜ì—¬ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ì§€ë§Œ ë¹ ì ¸ ìˆëŠ” ì¡°í•­ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
3. **ê°œì„  ë°©ì•ˆ ë° ëŒ€ì•ˆ ì œì‹œ**: ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ë©´ ì¢‹ì„ì§€ ëŒ€ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
4. **ì¢…í•©ì ì¸ ë²•ë¥  ìë¬¸**: ê³„ì•½ ì „ë°˜ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ê³¼ ì¶”ê°€ì ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”.
"""
        )

        # 3) context í…ìŠ¤íŠ¸í™” ìœ í‹¸
        def docs_to_text(docs):
            try:
                return "\n\n---\n\n".join(getattr(d, "page_content", str(d)) for d in docs)
            except Exception:
                return str(docs)

        # 4) ë‹µë³€ ìƒì„± ì²´ì¸ (ì¶œë ¥ì— ê·¼ê±° ì¸ìš© ìœ ë„)
        chain = (
            {
                "context": RETRIEVER | RunnableLambda(docs_to_text),
                "contract": RunnablePassthrough()
            }
            | prompt
            | ChatUpstage(model="solar-pro2", reasoning_effort="high")
            | StrOutputParser()
        )

        # 5) í† í° ì œí•œ í™•ì¸ ë° í…ìŠ¤íŠ¸ ë¶„í• 
        estimated_tokens = len(contract_text) // 4
        print(f"ğŸ“Š ê³„ì•½ì„œ í† í° ìˆ˜: ì•½ {estimated_tokens} í† í°")
        
        # RAG ê²€ìƒ‰ ê²°ê³¼ì˜ í† í° ìˆ˜ë„ ê³ ë ¤í•˜ì—¬ ë” ë‚®ì€ ì„ê³„ê°’ ì‚¬ìš©
        if estimated_tokens > 2000:  # RAG contextë¥¼ ê³ ë ¤í•˜ì—¬ 2000ìœ¼ë¡œ ë‚®ì¶¤
            print(f"âš ï¸ í† í° ìˆ˜ ì´ˆê³¼ ê°ì§€: ì•½ {estimated_tokens} í† í° (ì œí•œ: 4000)")
            print("ğŸ“ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤...")
            
            # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
            text_chunks = split_text_for_analysis(contract_text, max_tokens=2000)
            print(f"ğŸ“‹ ì´ {len(text_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
            
            # ê° ì²­í¬ë³„ë¡œ ë¶„ì„ ìˆ˜í–‰
            all_analyses = []
            for i, chunk in enumerate(text_chunks, 1):
                print(f"ğŸ” ì²­í¬ {i}/{len(text_chunks)} ë¶„ì„ ì¤‘...")
                try:
                    # RAG ì²´ì¸ ëŒ€ì‹  ë‹¨ìˆœ ë¶„ì„ ì‚¬ìš©
                    simple_prompt = ChatPromptTemplate.from_template(
                        """í•œêµ­ ë¶€ë™ì‚° ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ [ê³„ì•½ì„œ]ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

[ê³„ì•½ì„œ]
{contract}

ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì¤‘ì ì ìœ¼ë¡œ, ì„ì°¨ì¸ì˜ ì…ì¥ì—ì„œ ì´í•´í•˜ê¸° ì‰½ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í•­ëª©ì„ ë‚˜ëˆ„ì–´ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­**: ë…ì†Œ ì¡°í•­ì´ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ì‘ìš©í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ì§šì–´ì£¼ì„¸ìš”.
2. **ëˆ„ë½ëœ ì¤‘ìš” ì¡°í•­**: ì„ì°¨ì¸ ë³´í˜¸ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ì§€ë§Œ ë¹ ì ¸ ìˆëŠ” ì¡°í•­ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
3. **ê°œì„  ë°©ì•ˆ ë° ëŒ€ì•ˆ ì œì‹œ**: ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ë©´ ì¢‹ì„ì§€ ëŒ€ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
4. **ì¢…í•©ì ì¸ ë²•ë¥  ìë¬¸**: ê³„ì•½ ì „ë°˜ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ê³¼ ì¶”ê°€ì ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”.
"""
                    )
                    simple_chain = simple_prompt | ChatUpstage(model="solar-pro2", reasoning_effort="high") | StrOutputParser()
                    chunk_result = simple_chain.invoke({"contract": chunk})
                    all_analyses.append(f"## ì²­í¬ {i} ë¶„ì„ ê²°ê³¼\n\n{chunk_result}")
                except Exception as chunk_error:
                    print(f"âš ï¸ ì²­í¬ {i} ë¶„ì„ ì‹¤íŒ¨: {chunk_error}")
                    all_analyses.append(f"## ì²­í¬ {i} ë¶„ì„ ì‹¤íŒ¨\n\nì˜¤ë¥˜: {chunk_error}")
            
            # ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•©
            analysis_result = "\n\n---\n\n".join(all_analyses)
            print("âœ… ëª¨ë“  ì²­í¬ ë¶„ì„ ì™„ë£Œ ë° í†µí•©")
            
            # Groundedness CheckëŠ” ì „ì²´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ìˆ˜í–‰
            try:
                groundedness_result = groundedness_checker.invoke({"text": contract_text})
            except Exception as ge:
                print(f"âš ï¸ Groundedness Check ì‹¤íŒ¨: {ge}")
                groundedness_result = None
        else:
            # ì¼ë°˜ì ì¸ ë¶„ì„ ìˆ˜í–‰
            print(f"âœ… í† í° ìˆ˜ í™•ì¸: ì•½ {estimated_tokens} í† í° (ì œí•œ ë‚´)")
            
            try:
                # RAG ë‹µë³€ê³¼ Groundedness Check ë³‘í–‰ ì²´ì¸
                rag_chain_with_check = RunnablePassthrough.assign(
                    context=itemgetter("contract") | RunnableLambda(build_grounded_context_for_contract),
                    answer=itemgetter("contract") | chain
                ).assign(
                    groundedness=groundedness_checker
                )

                # ì‹¤í–‰ ë° ê²°ê³¼ ì¶”ì¶œ
                result_dict = rag_chain_with_check.invoke({"contract": contract_text})
                analysis_result = result_dict.get("answer", "")
                groundedness_result = result_dict.get("groundedness", None)
            except Exception as e:
                print(f"âš ï¸ RAG ë¶„ì„ ì‹¤íŒ¨, ë‹¨ìˆœ ë¶„ì„ìœ¼ë¡œ ì „í™˜: {e}")
                # RAG ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ë¶„ì„ìœ¼ë¡œ ì „í™˜
                simple_prompt = ChatPromptTemplate.from_template(
                    """í•œêµ­ ë¶€ë™ì‚° ë²•ë¥  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ [ê³„ì•½ì„œ]ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.

[ê³„ì•½ì„œ]
{contract}

ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì¤‘ì ì ìœ¼ë¡œ, ì„ì°¨ì¸ì˜ ì…ì¥ì—ì„œ ì´í•´í•˜ê¸° ì‰½ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í•­ëª©ì„ ë‚˜ëˆ„ì–´ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•œ ì¡°í•­**: ë…ì†Œ ì¡°í•­ì´ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ì„ì°¨ì¸ì—ê²Œ ë¶ˆë¦¬í•˜ê²Œ ì‘ìš©í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ì§šì–´ì£¼ì„¸ìš”.
2. **ëˆ„ë½ëœ ì¤‘ìš” ì¡°í•­**: ì„ì°¨ì¸ ë³´í˜¸ë¥¼ ìœ„í•´ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ì§€ë§Œ ë¹ ì ¸ ìˆëŠ” ì¡°í•­ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
3. **ê°œì„  ë°©ì•ˆ ë° ëŒ€ì•ˆ ì œì‹œ**: ë°œê²¬ëœ ë¬¸ì œì ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–»ê²Œ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ê°€í•˜ë©´ ì¢‹ì„ì§€ ëŒ€ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
4. **ì¢…í•©ì ì¸ ë²•ë¥  ìë¬¸**: ê³„ì•½ ì „ë°˜ì— ëŒ€í•œ ì¢…í•©ì ì¸ ì˜ê²¬ê³¼ ì¶”ê°€ì ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”.
"""
                )
                simple_chain = simple_prompt | ChatUpstage(model="solar-pro2", reasoning_effort="high") | StrOutputParser()
                analysis_result = simple_chain.invoke({"contract": contract_text})
                groundedness_result = None

        print("\n" + "="*50)
        print("ğŸ•µï¸  [ê³„ì•½ì„œ ë¶„ì„] Groundedness Check ê²°ê³¼ (í„°ë¯¸ë„ ì „ìš©)")
        if isinstance(groundedness_result, dict):
            score = groundedness_result.get("binary_score") or groundedness_result.get("score") or groundedness_result.get("result")
            reason = groundedness_result.get("reason") or groundedness_result.get("explanation")
        else:
            # ê°ì²´ë‚˜ ë¬¸ìì—´ ë“± ë‹¤ì–‘í•œ í˜•íƒœ ë°©ì–´ì  ì²˜ë¦¬
            score = getattr(groundedness_result, "binary_score", str(groundedness_result))
            reason = getattr(groundedness_result, "reason", "")
        print(f" - ì‚¬ì‹¤ ê¸°ë°˜ ì ìˆ˜: {score} ({'ê·¼ê±° ìˆìŒ' if str(score).lower().strip() == 'grounded' else 'ê·¼ê±° ì—†ìŒ'})")
        if reason:
            print(f" - ì´ìœ : {reason}")
        print("="*50 + "\n")

        return {"analysis": analysis_result}
    except Exception as e:
        print(f"âŒ Groundedness Check ë˜ëŠ” AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        return {"analysis": f"âŒ AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}

def deepl_translate_text(text, target_lang):
    if not DEEPL_API_KEY:
        lang_names = {"EN": "ì˜ì–´", "JA": "ì¼ë³¸ì–´", "ZH": "ì¤‘êµ­ì–´", "UK": "ìš°í¬ë¼ì´ë‚˜ì–´", "VI": "ë² íŠ¸ë‚¨ì–´"}
        return f"[{lang_names.get(target_lang, target_lang)} ë²ˆì—­ ê¸°ëŠ¥]\n\nDeepL API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ì‹¤ì œ ë²ˆì—­ì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\n\nì›ë³¸ í…ìŠ¤íŠ¸:\n{text}..."
    try:
        headers = {"Authorization": f"DeepL-Auth-Key {DEEPL_API_KEY}"}
        data = {"text": [text], "target_lang": target_lang}
        response = requests.post(DEEPL_API_URL, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        return response.json()["translations"][0]["text"]
    except Exception as e:
        return f"ë²ˆì—­ ì˜¤ë¥˜: {e}\n\nì›ë³¸ í…ìŠ¤íŠ¸:\n{text}..."

def split_text_for_tts(text, max_bytes=4500):
    if len(text.encode('utf-8')) <= max_bytes:
        return [text]
    chunks, current_chunk = [], ""
    sentences = re.split(r'(?<=[.!?ë‹¤])\s+', text)
    for sentence in sentences:
        test_chunk = current_chunk + sentence + " "
        if len(test_chunk.encode('utf-8')) <= max_bytes:
            current_chunk = test_chunk
        else:
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
            if len(sentence.encode('utf-8')) > max_bytes:
                words = sentence.split()
                temp_chunk = ""
                for word in words:
                    test_word_chunk = temp_chunk + word + " "
                    if len(test_word_chunk.encode('utf-8')) <= max_bytes:
                        temp_chunk = test_word_chunk
                    else:
                        if temp_chunk.strip():
                            chunks.append(temp_chunk.strip())
                        temp_chunk = word + " "
                current_chunk = temp_chunk
            else:
                current_chunk = sentence + " "
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    return chunks

def generate_report(file_name, rule_analysis, ai_analysis):
    score = rule_analysis['safety_score']
    if score >= 80:
        safety_grade = f"âœ… ë§¤ìš° ì•ˆì „ ({score}ì )"
    elif score >= 50:
        safety_grade = f"âš ï¸ ë³´í†µ ({score}ì )"
    elif score >= 0:
        safety_grade = f"ğŸš¨ ìœ„í—˜! ({score}ì )"
    else:
        safety_grade = "âš ï¸ ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜"
    alerts_text = "\n".join([f"- {alert}" for alert in rule_analysis['alerts']])
    return f"""# ğŸ  AI ë¶€ë™ì‚° ê³„ì•½ì„œ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸

## ğŸ“‹ ë¶„ì„ ëŒ€ìƒ
**íŒŒì¼**: ğŸ“„ {file_name}

## ğŸ•µï¸ ê³„ì•½ì„œ ì•ˆì „ë„ ê²€ì‚¬
**ì¢…í•© ì•ˆì „ë„**: {safety_grade}

{alerts_text}

## ğŸ§  AI ì‹¬ì¸µ ë¶„ì„
{ai_analysis['analysis']}

---
***ë³¸ ë¶„ì„ì€ ì°¸ê³ ìš©ì´ë©° ë²•ì  íš¨ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì • ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.***
"""

def extract_clean_text_from_html(html_content: str) -> str:
    """HTMLì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (CSS, íƒœê·¸ ì œê±°)"""
    # CSS ìŠ¤íƒ€ì¼ ë¸”ë¡ ì™„ì „ ì œê±°
    text = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL)
    # script íƒœê·¸ë„ ì œê±°
    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
    # head íƒœê·¸ ì „ì²´ ì œê±°
    text = re.sub(r'<head[^>]*>.*?</head>', '', text, flags=re.DOTALL)
    
    # ë¸”ë¡ íƒœê·¸ë¥¼ ê°œí–‰ìœ¼ë¡œ ë³€í™˜
    text = re.sub(r'</?(h1|h2|h3|h4|h5|h6|p|div|section|article|header|footer|li)>', '\n', text)
    text = re.sub(r'</?(ul|ol)>', '\n\n', text)
    text = re.sub(r'<br\s*/?>', '\n', text)
    
    # ë‚˜ë¨¸ì§€ ëª¨ë“  HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&amp;', '&')
    
    # ì—°ì†ëœ ê³µë°±ê³¼ ê°œí–‰ ì •ë¦¬
    text = re.sub(r'\n\s*\n', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì •ë¦¬
    lines = [line.strip() for line in text.split('\n')]
    clean_lines = []
    for line in lines:
        if line or (clean_lines and clean_lines[-1]):  # ì—°ì†ëœ ë¹ˆ ì¤„ ë°©ì§€
            clean_lines.append(line)
    
    return '\n'.join(clean_lines).strip()

def convert_emoji_to_text(text: str) -> str:
    """ì´ëª¨ì§€ë¥¼ í•œê¸€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (PNG ìƒì„±ìš©)"""
    emoji_map = {
        'ğŸ ': '[ì§‘]', 'ğŸ“‹': '[ë¬¸ì„œ]', 'ğŸ”': '[ê²€ìƒ‰]', 'ğŸ“Š': '[ì°¨íŠ¸]', 
        'ğŸ’¬': '[ì±„íŒ…]', 'ğŸ¤–': '[AI]', 'ğŸ“„': '[íŒŒì¼]', 'ğŸ“…': '[ë‚ ì§œ]',
        'ğŸš¨': '[ê²½ê³ ]', 'âš ï¸': '[ì£¼ì˜]', 'âœ…': '[í™•ì¸]', 'ğŸ’¡': '[ì•„ì´ë””ì–´]',
        'ğŸ•µï¸': '[íƒì •]', 'ğŸŒ': '[ì§€êµ¬]', 'ğŸ§': '[í—¤ë“œí°]', 'ğŸ“¸': '[ì¹´ë©”ë¼]',
        'ğŸ—‘ï¸': '[íœ´ì§€í†µ]', 'ğŸ“¤': '[ì—…ë¡œë“œ]', 'ğŸ¢': '[ë¹Œë”©]', 'ğŸ“': '[ë©”ëª¨]',
        'ğŸ§ ': '[ë‡Œ]', 'ğŸ‘': '[ì¢‹ì•„ìš”]', 'âŒ': '[X]', 'â­': '[ë³„]',
        'ğŸ“': '[í´ë¦½]', 'ğŸ”—': '[ë§í¬]', 'ğŸ“Œ': '[í•€]', 'ğŸ§¾': '[ì˜ìˆ˜ì¦]',
        'ğŸ“˜': '[íŒŒë€ì±…]', 'ğŸ“™': '[ì£¼í™©ì±…]', 'ğŸ“—': '[ì´ˆë¡ì±…]', 'ğŸ“•': '[ë¹¨ê°„ì±…]',
        'ğŸ”¥': '[ë¶ˆ]', 'âœ¨': '[ë°˜ì§ì„]', 'ğŸ“ˆ': '[ìƒìŠ¹ì°¨íŠ¸]', 'ğŸ“‰': '[í•˜ë½ì°¨íŠ¸]',
        'ğŸ¢': '[ê±°ë¶ì´]', 'ğŸ¡': '[ì£¼íƒ]', 'ğŸ’°': '[ëˆ]', 'ğŸ“‹': '[ì²´í¬ë¦¬ìŠ¤íŠ¸]',
        'ğŸ”': '[ìë¬¼ì‡ ]', 'ğŸ“œ': '[ê³„ì•½ì„œ]', 'âš–ï¸': '[ì €ìš¸]', 'ğŸ›ï¸': '[ë²•ì›]'
    }
    
    for emoji, text_replacement in emoji_map.items():
        text = text.replace(emoji, text_replacement)
    
    # ë‚¨ì€ ì´ëª¨ì§€ë“¤ì„ ì¼ë°˜ì ì¸ íŒ¨í„´ìœ¼ë¡œ ì œê±°í•˜ê±°ë‚˜ ë³€í™˜
    text = re.sub(r'[\U0001F600-\U0001F64F]', '[ì´ëª¨ì§€]', text)  # ê°ì • ì´ëª¨ì§€
    text = re.sub(r'[\U0001F300-\U0001F5FF]', '[ê¸°í˜¸]', text)    # ê¸°í˜¸ ì´ëª¨ì§€
    text = re.sub(r'[\U0001F680-\U0001F6FF]', '[êµí†µ]', text)    # êµí†µ ì´ëª¨ì§€
    text = re.sub(r'[\U0001F1E0-\U0001F1FF]', '[êµ­ê¸°]', text)    # êµ­ê°€ ì´ëª¨ì§€
    
    return text

def detect_language_code(text: str, translate_lang: str) -> str:
    """í…ìŠ¤íŠ¸ì™€ ë²ˆì—­ ì–¸ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ì–¸ì–´ ì½”ë“œ ë°˜í™˜"""
    if translate_lang == "ì›ë³¸":
        # í•œê¸€ì´ ë§ìœ¼ë©´ KO, ì˜ì–´ê°€ ë§ìœ¼ë©´ EN ë“±
        if len(re.findall(r'[ê°€-í£]', text)) > len(text) * 0.3:
            return 'KO'
        elif len(re.findall(r'[a-zA-Z]', text)) > len(text) * 0.5:
            return 'EN'
        # ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ê°ì§€ ë¡œì§ ì¶”ê°€
        elif len(re.findall(r'[\u3040-\u30ff]', text)) > len(text) * 0.3: # Hiragana/Katakana
            return 'JA'
        elif len(re.findall(r'[\u4e00-\u9fff]', text)) > len(text) * 0.3: # CJK Unified Ideographs
            return 'ZH'
        else:
            return 'KO'  # ê¸°ë³¸ê°’
    else:
        return translate_lang

def html_to_png_downloadable(html_content: str, filename_prefix="report_html", lang_code_override: str | None = None):
    """HTMLì„ PNGë¡œ ì €ì¥ - ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ê¹”ë”í•˜ê²Œ ì €ì¥ (ì´ëª¨ì§€ ì§€ì›)"""
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # HTMLì—ì„œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    clean_text = extract_clean_text_from_html(html_content)
    
    # ì–¸ì–´ ê°ì§€ (ë˜ëŠ” í˜¸ì¶œë¶€ì—ì„œ override)
    if lang_code_override:
        lang_code = lang_code_override
    else:
        lang_code = 'KO'
        # ë²ˆì—­ ê²°ê³¼ì— í¬í•¨ëœ ì–¸ì–´ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ ì½”ë“œ ì„¤ì •
        if any(keyword in clean_text for keyword in ['Translation Result', 'English', 'ì˜ì–´']):
            lang_code = 'EN'
        elif any(keyword in clean_text for keyword in ['ç¿»è¨³çµæœ', 'æ—¥æœ¬èª', 'ì¼ë³¸ì–´']):
            lang_code = 'JA'
        elif any(keyword in clean_text for keyword in ['ç¿»è¯‘ç»“æœ', 'ä¸­æ–‡', 'ì¤‘êµ­ì–´']):
            lang_code = 'ZH'
        elif any(keyword in clean_text for keyword in ['Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ñƒ', 'ìš°í¬ë¼ì´ë‚˜ì–´']) or any(char in clean_text for char in "Ğ°Ğ±Ğ²Ğ³Ğ´ĞµÑ‘Ğ¶Ğ·Ğ¸Ğ¹ĞºĞ»Ğ¼Ğ½Ğ¾Ğ¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑÑ"):
            lang_code = 'UK'
        elif any(keyword in clean_text for keyword in ['Káº¿t quáº£ dá»‹ch', 'ë² íŠ¸ë‚¨ì–´']):
            lang_code = 'VI'
    
    # PILë¡œ ê¹”ë”í•œ ì´ë¯¸ì§€ ìƒì„± (ì´ëª¨ì§€ í¬í•¨)
    img = create_clean_report_image(clean_text, filename_prefix, lang_code)
    out_path = Path(tempfile.gettempdir()) / f"{filename_prefix}_{ts}.png"
    img.save(out_path, format='PNG', quality=95, optimize=True)
    return str(out_path)

def create_report_image(report_text, title="AI ê³„ì•½ì„œ ë¶„ì„ ë¦¬í¬íŠ¸", lang="ko"):
    """ê¸°ì¡´ PIL ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ - ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í•˜ìœ„ í˜¸í™˜ìš©ìœ¼ë¡œë§Œ ìœ ì§€)"""
    return create_clean_report_image(report_text, "legacy_report")

def wrap_chat_html(answer_html_or_md: str, title="ğŸ¤– AI ë‹µë³€"):
    content = md_to_html(answer_html_or_md)
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    html = f"""
    <html>
      <head>
        {EMBED_HEAD}
        <style>{EMBED_CSS}</style>
      </head>
      <body>
        <div class="report-wrap">
          <article class="report-card">
            <header class="report-header" style="background: linear-gradient(135deg, #667eea, #764ba2);">
              <h1>{title}</h1>
              <div class="meta">ğŸ“… ìƒì„±ì¼ì‹œ: {timestamp}</div>
            </header>
            <section class="report-section">
              <div>{content}</div>
            </section>
          </article>
        </div>
      </body>
    </html>
    """
    return html

# Gradioìš© functions
def analyze_contract(file, progress=gr.Progress(track_tqdm=True)):
    if file is None:
        return "âŒ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", "", "", ""
    try:
        progress(0.1, desc="[ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸª„ğŸª„ğŸª„ğŸª„ğŸª„..")
        text, status = extract_text_from_file(file.name)
        if not text:
            return f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {status}", "", "", ""

        progress(0.4, desc="ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸª„ğŸª„ğŸª„ğŸª„ğŸª„...")
        rule_analysis = perform_rule_based_analysis(text) # <<< ì„ëŒ€ì¸ ì¡°íšŒê°€ í¬í•¨ëœ í•¨ìˆ˜ í˜¸ì¶œ

        progress(0.7, desc="ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸª„ğŸª„ğŸª„ğŸª„ğŸª„.")
        ai_analysis = perform_ai_analysis(text)

        progress(0.9, desc="[ğŸ¢ğŸ¢ğŸ¢ğŸ¢ğŸª„ğŸª„ğŸª„ğŸª„ğŸª„...")
        md_report = generate_report(os.path.basename(file.name), rule_analysis, ai_analysis)
        html_report = render_report_html(os.path.basename(file.name), rule_analysis, ai_analysis)

        return html_report, text, md_report, html_report
    except Exception as e:
        return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "", "", ""

def chat_with_ai(message, history):
    """RAGì™€ Groundedness Checkë¥¼ ì‚¬ìš©í•˜ì—¬ ë²•ë¥  ìƒë‹´ ì±„íŒ…ì„ ì§„í–‰í•©ë‹ˆë‹¤."""
    if not message.strip():
        return history, ""
    
    # RAG ê²€ìƒ‰ê¸°(RETRIEVER)ê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not RETRIEVER:
        err_msg = "âš ï¸ AI ìƒë‹´ ì—”ì§„(RAG)ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ë‹¤ì‹œ ì‹œì‘í•˜ê±°ë‚˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        history.append((message, err_msg))
        return history, ""

    try:
        # 1) Groundedness Check ê°ì²´ ìƒì„±
        groundedness_checker = UpstageGroundednessCheck()

        # 2) í”„ë¡¬í”„íŠ¸ ì •ì˜
        prompt = ChatPromptTemplate.from_template(
            """ë‹¹ì‹ ì€ í•œêµ­ ë¶€ë™ì‚° ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ì°¸ê³  ìë£Œ]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [ì§ˆë¬¸]ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”. ë²•ì  íš¨ë ¥ì´ ì—†ìŒì„ ëª…ì‹œí•˜ê³  ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œìœ í•˜ëŠ” ë‚´ìš©ì„ í¬í•¨í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ ê³§ë°”ë¡œ í•µì‹¬ ë‚´ìš©ë¶€í„° ì‹œì‘í•˜ê³ , RAG/ì°¸ê³  ìë£Œë¥¼ ì–¸ê¸‰í•˜ëŠ” ì„œë¬¸ì´ë‚˜ ë©”íƒ€ ë¬¸êµ¬(ì˜ˆ: 'ì£¼ì–´ì§„ [ì°¸ê³  ìë£Œ]ì™€ ê´€ë ¨ ë²•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤')ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ê° í•µì‹¬ ì£¼ì¥ë§ˆë‹¤ ê´€ë ¨ [ì°¸ê³  ìë£Œ]ë‚˜ ì¡°ë¬¸/ë¬¸êµ¬ë¥¼ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ê°„ëµíˆ ì¸ìš©í•˜ê³  ë”°ì˜´í‘œë¡œ í‘œì‹œí•˜ì„¸ìš”.

[ì°¸ê³  ìë£Œ]
{context}

[ì§ˆë¬¸]
{question}
"""
        )

        # 3) ë‹µë³€ ìƒì„± ì²´ì¸ (ì¶œë ¥ì— ê·¼ê±° ì¸ìš© ìœ ë„)
        chain = (
            {
                "context": RETRIEVER | RunnableLambda(docs_to_text),
                "question": RunnablePassthrough()
            }
            | prompt
            | ChatUpstage(model="solar-pro2", reasoning_effort="high")
            | StrOutputParser()
        )

        # 4) RAG ë‹µë³€ê³¼ Groundedness Check ë³‘í–‰
        rag_chain_with_check = RunnablePassthrough.assign(
            context=itemgetter("question") | RunnableLambda(build_grounded_context_for_question),
            answer=itemgetter("question") | chain
        ).assign(
            groundedness=groundedness_checker
        )

        # 5) ì‹¤í–‰ ë° í„°ë¯¸ë„ ì¶œë ¥
        result_dict = rag_chain_with_check.invoke({"question": message})
        response = result_dict.get("answer", "")
        groundedness_result = result_dict.get("groundedness", None)

        print("\n" + "="*50)
        print("ğŸ’¬ [ì‹¤ì‹œê°„ ìƒë‹´] Groundedness Check ê²°ê³¼ (í„°ë¯¸ë„ ì „ìš©)")
        if isinstance(groundedness_result, dict):
            score = groundedness_result.get("binary_score") or groundedness_result.get("score") or groundedness_result.get("result")
            reason = groundedness_result.get("reason") or groundedness_result.get("explanation")
        else:
            score = getattr(groundedness_result, "binary_score", str(groundedness_result))
            reason = getattr(groundedness_result, "reason", "")
        print(f" - ì‚¬ì‹¤ ê¸°ë°˜ ì ìˆ˜: {score} ({'ê·¼ê±° ìˆìŒ' if str(score).lower().strip() == 'grounded' else 'ê·¼ê±° ì—†ìŒ'})")
        if reason:
            print(f" - ì´ìœ : {reason}")
        print("="*50 + "\n")

        history.append((message, response))
        return history, response
    except Exception as e:
        err_msg = f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}"
        history.append((message, err_msg))
        return history, err_msg

# Gradio ì¸í„°í˜ì´ìŠ¤ 
def create_interface():
    with gr.Blocks(
        title="AI ë¶€ë™ì‚° ë²•ë¥  ë¹„ì„œ",
        theme=gr.themes.Soft(primary_hue="emerald", secondary_hue="green")
    ) as interface:
        # ë©”ì¸ í—¤ë” - ì•„ë¦„ë‹¤ìš´ ë””ìì¸
        with gr.Row():
            gr.HTML("""
            <div style="
                background: linear-gradient(135deg, #10b981 0%, #059669 50%, #047857 100%);
                border-radius: 20px;
                padding: 2rem;
                margin: 1rem 0;
                box-shadow: 0 10px 30px rgba(16, 185, 129, 0.3);
                text-align: center;
                position: relative;
                overflow: hidden;
            ">
                <!-- ë°°ê²½ ì¥ì‹ ìš”ì†Œ -->
                <div style="
                    position: absolute;
                    top: -50px;
                    right: -50px;
                    width: 100px;
                    height: 100px;
                    background: rgba(255, 255, 255, 0.1);
                    border-radius: 50%;
                "></div>
                <div style="
                    position: absolute;
                    bottom: -30px;
                    left: -30px;
                    width: 60px;
                    height: 60px;
                    background: rgba(255, 255, 255, 0.08);
                    border-radius: 50%;
                "></div>
                
                <!-- ë©”ì¸ ì œëª© -->
                <div style="
                    display: flex;
                    align-items: center;
                    justify-content: center;
                    gap: 1rem;
                    margin-bottom: 1rem;
                ">
                    <div style="
                        font-size: 3rem;
                        filter: drop-shadow(0 4px 8px rgba(0, 0, 0, 0.2));
                    "></div>
                    <div>
                        <h1 style="
                            color: white;
                            margin: 0;
                            font-size: 4.5rem;
                            font-weight: 700;
                            text-shadow: 0 2px 4px rgba(0, 0, 0, 0.3);
                            letter-spacing: -0.02em;
                        ">Shellter</h1>
                    </div>
                    <div style="
                        font-size: 3rem;
                        filter: drop-shadow(0 4px 8px rgba(0, 0, 0, 0.2));
                    "></div>
                </div>
                
                <!-- ì„œë¸Œ íƒ€ì´í‹€ -->
                <p style="
                    color: rgba(255, 255, 255, 0.95);
                    margin: 0;
                    font-size: 1.2rem;
                    font-weight: 400;
                    text-shadow: 0 1px 2px rgba(0, 0, 0, 0.2);
                    line-height: 1.5;
                ">ğŸ¢ ë¶€ë™ì‚° ê³„ì•½ì„œì˜ ìˆ¨ì€ ìœ„í—˜ì„ ì°¾ì•„ë‚´ê³ , ë‹¹ì‹ ì˜ ì†Œì¤‘í•œ ìì‚°ì„ ì§€ì¼œë“œë¦½ë‹ˆë‹¤.</p>
                
                <!-- ê¸°ëŠ¥ í•˜ì´ë¼ì´íŠ¸ -->
                <div style="
                    display: flex;
                    justify-content: center;
                    gap: 1.5rem;
                    margin-top: 1.5rem;
                    flex-wrap: wrap;
                ">
                </div>
            </div>
            """)

        # ë©”ì¸ ì»¨í…ì¸  
        with gr.Row():
            # ì™¼ìª½: íŒŒì¼ ë¶„ì„
            with gr.Column(scale=6):
                gr.Markdown("## ğŸ“‹ ê³„ì•½ì„œ ë¶„ì„")
                file_input = gr.File(
                    label="ğŸ“ ê³„ì•½ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
                    file_types=[".pdf", ".jpg", ".jpeg", ".png", ".doc", ".docx", ".hwp", ".txt"],
                    type="filepath"
                )
                with gr.Row():
                    analyze_btn = gr.Button("ğŸ” ë¶„ì„ ì‹œì‘", variant="primary", size="lg")
                    clear_btn = gr.Button("ğŸ—‘ï¸ ì´ˆê¸°í™”", variant="secondary")
                
                with gr.Accordion("ğŸŒ ë¶„ì„ ê²°ê³¼ ë¶€ê°€ê¸°ëŠ¥", open=False):
                    with gr.Row():
                        analysis_translate_lang = gr.Dropdown(
                            choices=[
                                ("ì›ë³¸", "ì›ë³¸"),
                                ("ì˜ì–´ ğŸ‡ºğŸ‡¸", "EN"),
                                ("ì¼ë³¸ì–´ ğŸ‡¯ğŸ‡µ", "JA"),
                                ("ì¤‘êµ­ì–´ ğŸ‡¨ğŸ‡³", "ZH"),
                                ("ìš°í¬ë¼ì´ë‚˜ì–´ ğŸ‡ºğŸ‡¦", "UK"),
                                ("ë² íŠ¸ë‚¨ì–´ ğŸ‡»ğŸ‡³", "VI")
                            ],
                            label="ì–¸ì–´ ì„ íƒ",
                            value="ì›ë³¸"
                        )
                        analysis_speech_lang = gr.Dropdown(
                            choices=[
                                ("í•œêµ­ì–´ ğŸ‡°ğŸ‡·", "KO"),
                                ("ì˜ì–´ ğŸ‡ºğŸ‡¸", "EN"),
                                ("ì¼ë³¸ì–´ ğŸ‡¯ğŸ‡µ", "JA"),
                                ("ì¤‘êµ­ì–´ ğŸ‡¨ğŸ‡³", "ZH"),
                                ("ìš°í¬ë¼ì´ë‚˜ì–´ ğŸ‡ºğŸ‡¦", "UK"),
                                ("ë² íŠ¸ë‚¨ì–´ ğŸ‡»ğŸ‡³", "VI")
                            ],
                            label="ìŒì„± ì–¸ì–´",
                            value="KO"
                        )
                    with gr.Row():
                        analysis_translate_btn = gr.Button("ğŸŒ ë²ˆì—­í•˜ê¸°", variant="secondary")
                        analysis_speech_btn = gr.Button("ğŸ§ ìŒì„± ìƒì„±", variant="secondary")
                        analysis_image_btn = gr.Button("ğŸ“ PNG ì €ì¥", variant="secondary")
                        analysis_translate_png_btn = gr.Button("ğŸ—‚ï¸ ë²ˆì—­ PNG", variant="secondary")

                    # ë²ˆì—­ ê²°ê³¼ë¥¼ HTMLë¡œ í‘œì‹œ
                    analysis_translation_output = gr.HTML(label="ë²ˆì—­ëœ ë¶„ì„ ê²°ê³¼", visible=True)
                    with gr.Row():
                        analysis_audio_output = gr.Audio(label="ë¶„ì„ ê²°ê³¼ ìŒì„±", type="filepath")
                        analysis_speech_status = gr.Textbox(label="ìŒì„± ìƒíƒœ", interactive=False)
                    with gr.Row():
                        analysis_image_download = gr.File(label="ğŸ“ ìƒì„±ëœ ë¦¬í¬íŠ¸ PNG", visible=True)
                        analysis_translate_image_download = gr.File(label="ğŸ—‚ï¸ ë²ˆì—­ ë¦¬í¬íŠ¸ PNG", visible=True)

            # ì˜¤ë¥¸ìª½: ì±„íŒ… ë° ë³´ê³ ì„œ
            with gr.Column(scale=6):
                gr.Markdown("## ğŸ•µï¸ AI ë¶„ì„ & ìƒë‹´")
                with gr.Tabs() as tabs:
                    with gr.TabItem("ğŸ“Š ë¶„ì„ ë³´ê³ ì„œ", id=0):
                        analysis_output_html = gr.HTML(
                           value="<div style='text-align: center; padding: 40px; color: #6b7280; border: 2px dashed #e5e7eb; border-radius: 12px;'><p>ğŸ“¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  <b>[ğŸ” ë¶„ì„ ì‹œì‘]</b> ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.</p></div>"
                        )
                    
                    with gr.TabItem("ğŸ’¬ ì‹¤ì‹œê°„ ìƒë‹´", id=1):
                        chatbot = gr.Chatbot(
                            value=[(None, "ì•ˆë…•í•˜ì„¸ìš”! ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.")],
                            height=400, show_label=False, container=True, show_copy_button=True,
                            bubble_full_width=False,
                        )
                        msg_input = gr.Textbox(placeholder="ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", show_label=False, container=False)
                        send_btn = gr.Button("ğŸ“¤ ì „ì†¡", variant="primary")
                        gr.Examples(
                            [
                                "ì „ì„¸ ê³„ì•½ ì‹œ ì£¼ì˜ì‚¬í•­ì€ ë­”ê°€ìš”?",
                                "ì „ì„¸ì§‘ì— í•˜ì(ê³°íŒ¡ì´, ëˆ„ìˆ˜ ë“±)ê°€ ìƒê¸°ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                                "ì§‘ì£¼ì¸ì´ ë°”ë€Œë©´ ê³„ì•½ì„œë¥¼ ë‹¤ì‹œ ì¨ì•¼ í•˜ë‚˜ìš”?",
                                "ì „ì„¸ ë§Œê¸°ì¸ë° ë³´ì¦ê¸ˆì„ ëª» ì¤€ë‹¤ í•˜ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
                                "ì´ ì§‘ì´ ê²½ë§¤ì— ë„˜ì–´ê°”ë‹¤ê³  í•˜ëŠ”ë° ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                                "ì „ì„¸ë³´ì¦ë³´í—˜, ì§€ê¸ˆì´ë¼ë„ ê°€ì…í•˜ë©´ ë³´í˜¸ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?"
                            ],
                            inputs=msg_input, label="ğŸ’¡ ì§ˆë¬¸ ì˜ˆì‹œ"
                        )
                        with gr.Accordion("ğŸŒ ì±„íŒ… ë‹µë³€ ë¶€ê°€ê¸°ëŠ¥", open=False):
                            with gr.Row():
                                chat_translate_lang = gr.Dropdown(
                                    choices=[
                                        ("ì›ë³¸", "ì›ë³¸"),
                                        ("ì˜ì–´ ğŸ‡ºğŸ‡¸", "EN"),
                                        ("ì¼ë³¸ì–´ ğŸ‡¯ğŸ‡µ", "JA"),
                                        ("ì¤‘êµ­ì–´ ğŸ‡¨ğŸ‡³", "ZH"),
                                        ("ìš°í¬ë¼ì´ë‚˜ì–´ ğŸ‡ºğŸ‡¦", "UK"),
                                        ("ë² íŠ¸ë‚¨ì–´ ğŸ‡»ğŸ‡³", "VI")
                                    ],
                                    label="ë²ˆì—­ ì–¸ì–´",
                                    value="ì›ë³¸"
                                )
                                chat_speech_lang = gr.Dropdown(
                                    choices=[
                                        ("í•œêµ­ì–´ ğŸ‡°ğŸ‡·", "KO"),
                                        ("ì˜ì–´ ğŸ‡ºğŸ‡¸", "EN"),
                                        ("ì¼ë³¸ì–´ ğŸ‡¯ğŸ‡µ", "JA"),
                                        ("ì¤‘êµ­ì–´ ğŸ‡¨ğŸ‡³", "ZH"),
                                        ("ìš°í¬ë¼ì´ë‚˜ì–´ ğŸ‡ºğŸ‡¦", "UK"),
                                        ("ë² íŠ¸ë‚¨ì–´ ğŸ‡»ğŸ‡³", "VI")
                                    ],
                                    label="ìŒì„± ì–¸ì–´",
                                    value="KO"
                                )
                            with gr.Row():
                                chat_translate_btn = gr.Button("ğŸŒ ë²ˆì—­", variant="secondary")
                                chat_speech_btn = gr.Button("ğŸ§ ìŒì„±", variant="secondary")
                                chat_image_btn = gr.Button("ğŸ“ PNG ì €ì¥", variant="secondary")
                                chat_translate_png_btn = gr.Button("ğŸ—‚ï¸  ë²ˆì—­ PNG", variant="secondary")
                            # ì±„íŒ… ë²ˆì—­ ê²°ê³¼ë„ HTMLë¡œ í‘œì‹œ
                            chat_translation_output = gr.HTML(label="ë²ˆì—­ëœ ë‹µë³€", visible=True)
                            chat_audio_output = gr.Audio(label="ë‹µë³€ ìŒì„±", type="filepath")
                            chat_speech_status = gr.Textbox(label="ìŒì„± ìƒíƒœ", interactive=False)
                            with gr.Row():
                                chat_image_download = gr.File(label="ğŸ“ ë‹µë³€ PNG", visible=True)
                                chat_translate_image_download = gr.File(label="ğŸ—‚ï¸  ë²ˆì—­ ë‹µë³€ PNG", visible=True)
                        

                    


        # ìƒíƒœ ê´€ë¦¬
        extracted_text = gr.State("")
        analysis_report_md = gr.State("")
        analysis_report_html_state = gr.State("")
        last_chat_response = gr.State("")
        analysis_translated_text = gr.State("")
        chat_translated_text = gr.State("")

        # ë²ˆì—­ í•¨ìˆ˜ (HTML í¬í•¨)
        def translate_analysis_with_html(report_md, lang):
            if not report_md.strip():
                return "<div style='padding: 20px; text-align: center; color: #6b7280;'>ë²ˆì—­í•  ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.</div>", ""
            if lang == "ì›ë³¸":
                return create_translated_html(report_md, "ì›ë³¸ ë¶„ì„ ê²°ê³¼"), report_md
            translated = deepl_translate_text(report_md, lang)
            lang_names = {"EN": "ì˜ì–´", "JA": "ì¼ë³¸ì–´", "ZH": "ì¤‘êµ­ì–´", "UK": "ìš°í¬ë¼ì´ë‚˜ì–´", "VI": "ë² íŠ¸ë‚¨ì–´"}
            title = f"{lang_names.get(lang, lang)} ë²ˆì—­ ê²°ê³¼"
            return create_translated_html(translated, title), translated

        def translate_chat_with_html(last_resp, lang):
            if not last_resp.strip():
                return "<div style='padding: 20px; text-align: center; color: #6b7280;'>ë²ˆì—­í•  ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.</div>", ""
            if lang == "ì›ë³¸":
                return create_translated_html(last_resp, "ì›ë³¸ ë‹µë³€"), last_resp
            translated = deepl_translate_text(last_resp, lang)
            lang_names = {"EN": "ì˜ì–´", "JA": "ì¼ë³¸ì–´", "ZH": "ì¤‘êµ­ì–´", "UK": "ìš°í¬ë¼ì´ë‚˜ì–´", "VI": "ë² íŠ¸ë‚¨ì–´"}
            title = f"{lang_names.get(lang, lang)} ë²ˆì—­ ë‹µë³€"
            return create_translated_html(translated, title), translated

        # ë²ˆì—­ PNG ì €ì¥ í•¨ìˆ˜ë“¤
        def save_analysis_translation_png(translated_text, translate_lang):
            if not translated_text.strip():
                return None
            # ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ê°ì‹¸ê³ , HTMLâ†’í…ìŠ¤íŠ¸ ì •ì œâ†’PNG íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
            lang_title = {"EN": "ì˜ì–´", "JA": "ì¼ë³¸ì–´", "ZH": "ì¤‘êµ­ì–´", "UK": "ìš°í¬ë¼ì´ë‚˜ì–´", "VI": "ë² íŠ¸ë‚¨ì–´"}
            html = create_translated_html(translated_text, f"{lang_title.get(translate_lang, translate_lang)} ë²ˆì—­ ê²°ê³¼")
            # íŒŒì¼ëª…ì— ì–¸ì–´ ì½”ë“œ í¬í•¨
            code = translate_lang if translate_lang in {"EN","JA","ZH","UK","VI"} else "ORIG"
            # HTMLâ†’í…ìŠ¤íŠ¸ ì •ì œ ë‚´ë¶€ì—ì„œ ì´ëª¨ì§€ í•œê¸€ ë³€í™˜(convert_emoji_to_text) ìˆ˜í–‰ë¨
            return html_to_png_downloadable(html, filename_prefix=f"analysis_translation_{code}", lang_code_override=(translate_lang if translate_lang != "ì›ë³¸" else "KO"))

        def save_chat_translation_png(translated_text, translate_lang):
            if not translated_text.strip():
                return None
            # ë²ˆì—­ í…ìŠ¤íŠ¸ë¥¼ HTMLë¡œ ê°ì‹¸ê³ , HTMLâ†’í…ìŠ¤íŠ¸ ì •ì œâ†’PNG íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
            lang_title = {"EN": "ì˜ì–´", "JA": "ì¼ë³¸ì–´", "ZH": "ì¤‘êµ­ì–´", "UK": "ìš°í¬ë¼ì´ë‚˜ì–´", "VI": "ë² íŠ¸ë‚¨ì–´"}
            html = create_translated_html(translated_text, f"{lang_title.get(translate_lang, translate_lang)} ë²ˆì—­ ë‹µë³€")
            # íŒŒì¼ëª…ì— ì–¸ì–´ ì½”ë“œ í¬í•¨
            code = translate_lang if translate_lang in {"EN","JA","ZH","UK","VI"} else "ORIG"
            # HTMLâ†’í…ìŠ¤íŠ¸ ì •ì œ ë‚´ë¶€ì—ì„œ ì´ëª¨ì§€ í•œê¸€ ë³€í™˜(convert_emoji_to_text) ìˆ˜í–‰ë¨
            return html_to_png_downloadable(html, filename_prefix=f"chat_translation_{code}", lang_code_override=(translate_lang if translate_lang != "ì›ë³¸" else "KO"))

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def clear_all():
            empty_html = "<div style='display:flex; justify-content:center; align-items:center; height:400px; border: 2px dashed #e5e7eb; border-radius: 20px;'><p style='color:#6b7280;'>ğŸ“¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  <b>[ğŸ” ë¶„ì„ ì‹œì‘]</b> ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.</p></div>"
            empty_translation = "<div style='padding: 20px; text-align: center; color: #6b7280;'>ë²ˆì—­í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.</div>"
            return (
                None, empty_html, empty_translation, None, "", 
                [(None, "ì•ˆë…•í•˜ì„¸ìš”! ë¶€ë™ì‚° ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.")], 
                None, "", "", empty_translation, "", None, None, None, None, None, "", "", gr.update(selected=0)
            )

        def analyze_and_store_report(file, progress=gr.Progress(track_tqdm=True)):
            html_report, text, md_report, html_pretty = analyze_contract(file, progress)
            return html_report, text, md_report, html_pretty, gr.update(selected=0)

        def store_chat_response(message, history):
            new_history, last_resp = chat_with_ai(message, history) # chat_with_aiëŠ” ì´ì œ RAG ê¸°ë°˜
            # ë§ˆì§€ë§‰ ë‹µë³€ë§Œ last_chat_response ìƒíƒœì— ì €ì¥
            if new_history and len(new_history) > 0:
                last_resp_text = new_history[-1][1]
            else:
                last_resp_text = ""
            return new_history, "", last_resp_text

        def generate_analysis_speech(report_md, lang, translate_lang):
            if not report_md.strip():
                return None, "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            
            speech_text_to_use = report_md

            # ë²ˆì—­ ì˜µì…˜ì´ 'ì›ë³¸'ì´ ì•„ë‹ ê²½ìš° ë²ˆì—­ì„ ë¨¼ì € ì‹œë„
            if translate_lang != "ì›ë³¸":
                lang_code_map = {"EN": "EN", "JA": "JA", "ZH": "ZH", "UK": "UK", "VI": "VI"}
                if lang in lang_code_map:
                    translated_output = deepl_translate_text(report_md, lang)
                    if "ë²ˆì—­ ì˜¤ë¥˜" not in translated_output:
                        speech_text_to_use = translated_output
            
            # ì–¸ì–´ ì½”ë“œë¥¼ ì§ì ‘ ì‚¬ìš© (ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹)
            return google_text_to_speech(speech_text_to_use, lang)

        def save_analysis_png(report_html):
            if not report_html:
                return None
            return html_to_png_downloadable(report_html, filename_prefix="analysis_report")

        def generate_chat_speech(last_resp, lang, translate_lang):
            if not last_resp.strip():
                return None, "ì±„íŒ… ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤."

            speech_text_to_use = last_resp

            # ë²ˆì—­ ì˜µì…˜ì´ 'ì›ë³¸'ì´ ì•„ë‹ ê²½ìš° ë²ˆì—­ì„ ë¨¼ì € ì‹œë„
            if translate_lang != "ì›ë³¸":
                lang_code_map = {"EN": "EN", "JA": "JA", "ZH": "ZH", "UK": "UK", "VI": "VI"}
                if lang in lang_code_map:
                    translated_output = deepl_translate_text(last_resp, lang)
                    if "ë²ˆì—­ ì˜¤ë¥˜" not in translated_output:
                        speech_text_to_use = translated_output

            # ì–¸ì–´ ì½”ë“œë¥¼ ì§ì ‘ ì‚¬ìš© (ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹)
            return google_text_to_speech(speech_text_to_use, lang)

        def save_chat_png(last_resp):
            if not last_resp.strip():
                return None
            html = wrap_chat_html(last_resp, title="ğŸ¤– AI ë‹µë³€")
            return html_to_png_downloadable(html, filename_prefix="chat_response")
        
        # ë°”ì¸ë”©
        analyze_btn.click(
            fn=analyze_and_store_report,
            inputs=[file_input],
            outputs=[analysis_output_html, extracted_text, analysis_report_md, analysis_report_html_state, tabs]
        )
        clear_btn.click(
            fn=clear_all,
            outputs=[
                file_input, analysis_output_html, analysis_translation_output, analysis_audio_output,
                analysis_speech_status, chatbot, chat_audio_output, chat_speech_status,
                msg_input, chat_translation_output, extracted_text, analysis_report_md,
                analysis_image_download, chat_image_download, last_chat_response, 
                analysis_translate_image_download, chat_translate_image_download,
                analysis_translated_text, chat_translated_text, tabs
            ]
        )
        analysis_translate_btn.click(
            fn=translate_analysis_with_html, 
            inputs=[analysis_report_md, analysis_translate_lang], 
            outputs=[analysis_translation_output, analysis_translated_text]
        )
        analysis_speech_btn.click(
            fn=generate_analysis_speech, 
            inputs=[analysis_report_md, analysis_speech_lang, analysis_translate_lang], 
            outputs=[analysis_audio_output, analysis_speech_status]
        )
        analysis_image_btn.click(
            fn=save_analysis_png, 
            inputs=[analysis_report_html_state], 
            outputs=[analysis_image_download]
        )
        analysis_translate_png_btn.click(
            fn=save_analysis_translation_png,
            inputs=[analysis_translated_text, analysis_translate_lang],
            outputs=[analysis_translate_image_download]
        )
        send_btn.click(
            fn=store_chat_response, 
            inputs=[msg_input, chatbot], 
            outputs=[chatbot, msg_input, last_chat_response]
        )
        msg_input.submit(
            fn=store_chat_response, 
            inputs=[msg_input, chatbot], 
            outputs=[chatbot, msg_input, last_chat_response]
        )
        chat_translate_btn.click(
            fn=translate_chat_with_html, 
            inputs=[last_chat_response, chat_translate_lang], 
            outputs=[chat_translation_output, chat_translated_text]
        )
        chat_speech_btn.click(
            fn=generate_chat_speech, 
            inputs=[last_chat_response, chat_speech_lang, chat_translate_lang], 
            outputs=[chat_audio_output, chat_speech_status]
        )
        chat_image_btn.click(
            fn=save_chat_png, 
            inputs=[last_chat_response], 
            outputs=[chat_image_download]
        )
        chat_translate_png_btn.click(
            fn=save_chat_translation_png,
            inputs=[chat_translated_text, chat_translate_lang],
            outputs=[chat_translate_image_download]
        )

    return interface

def main():
    print("ğŸ¢ğŸ¢ğŸ¢ğŸ¢ AI ë¶€ë™ì‚° ë²•ë¥  ë¹„ì„œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…) ë‹¤êµ­ì–´ í°íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
    setup_fonts()
    
    # 2. (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…) AIì˜ ì§€ì‹ ë² ì´ìŠ¤(Vector DB) êµ¬ì¶•
    build_ai_brain_if_needed()

    # 3. (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…) RAG ê²€ìƒ‰ê¸°(Retriever) ì´ˆê¸°í™”
    initialize_retriever()
    
    try:
        # 4. Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
        app = create_interface()
        print("âœ… ì¸í„°í˜ì´ìŠ¤ ìƒì„± ì™„ë£Œ. ì›¹ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        app.launch(server_name="0.0.0.0", server_port=7860, share=True, favicon_path="./Image/logo.png")
    except Exception as e:
        print(f"âŒ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()